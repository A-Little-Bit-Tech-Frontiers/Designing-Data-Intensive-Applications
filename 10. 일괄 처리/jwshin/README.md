# 10장. 일괄 처리

## 1. 개요

- 데이터를 처리하는 시스템은 크게 3가지 유형이 있다.
    - **온라인 서비스(요청–응답)**: 사용자가 요청하면 즉시 처리해 **응답 시간**을 최소화. 가용성 중시.
    - **일괄 처리(Batch)**: 큰 입력을 받아 한꺼번에 계산해 **결과 데이터**를 생성. **처리량(throughput)** 중시.
    - **스트림 처리(Stream)**: 끝이 없는 이벤트 흐름을 **발생 직후** 처리. **지연(latency)** 을 낮추지만 요청-응답은 아님.
- 일괄 처리 알고리즘인 맵리듀스(MapReduce)는 “구글을 대규모로 확장 가능하게 만든 알고리즘”이라고 불릴 만큼 매우 중요한 구성요소다.
- 이번 장은 **유닉스 철학 → MapReduce/HDFS → 조인·스큐·워크플로 → 결과 적재 → 데이터플로 엔진(Spark/Tez/Flink) → 반복/그래프(Pregel)** 순으로 현대 배치 처리를 설명한다.

## 2. 유닉스 도구로 일괄 처리하기

- 예제: 웹 서버로 들어온 요청이 처리될 때마다 로그 파일에 한줄씩 추가되는 서버
    
    <img width="566" height="118" alt="image" src="https://github.com/user-attachments/assets/68c2f7f6-c35c-4ffd-bef0-90f53eed9f1d" />
    
- 로그 형식 정의
    
    <img width="432" height="70" alt="image" src="https://github.com/user-attachments/assets/097fc00f-e347-478d-a46b-8b91be4a2244" />

- 해석:
    - 서버는 2015년 2월 27일 UTC 17시 55분 11초에 클라이언트 IP 주소 216.58.210.78로부터 /css/typography.css 파일에 대한 요청을 받았다.
    - 비인증 사용자여서 $remote_user가 하이픈(-)으로 기록되었다.
    - 응답 상태는 200으로 요청이 성공했고 응답 크기는 3377바이트였다.
    - 웹 브라우저는 크롬40이고 파일이 [`http://martin.kleppmann.com/`](http://martin.kleppmann.com/) 이라는 URL에서 참조됐다.

### 2-1. 단순 로그 분석

- 유닉스 도구만으로도 **상위 인기 URL 5개** 같은 간단한 보고서를 빠르게 만들 수 있다. 수 GB 로그도 수 초 내 처리 가능하며, 조합을 바꾸기 쉬워 실험에 유리하다.
- 예제: 기본 유닉스 도구를 사용해서 웹사이트에서 가장 인기 많은 페이지 5개를 뽑는다.
    
    <img width="266" height="166" alt="image" src="https://github.com/user-attachments/assets/b9fa817f-15fd-4c3b-a101-df2e97f2a05d" />

- 결과
    
    <img width="513" height="140" alt="image" src="https://github.com/user-attachments/assets/4eda5328-c264-404c-ac92-c01dfcd7f8b8" />
    
- 유닉스 명령어를 도구를 사용한 이런 방식은 수 GB의 로그 파일을 수 초 내로 처리할 수 있고, 필요에 따라 분석 방법을 수정하기 쉽다.

### 2-2. 연쇄 명령 대 맞춤형 프로그램

- 유닉스 연쇄 명령 대신 같은 작업을 하는 간단한 프로그램을 작성할 수도 잇다.
- 예제: 루비를 사용해서 유닉스 도구 대체하기
    
    <img width="534" height="298" alt="image" src="https://github.com/user-attachments/assets/8ecf5b0c-d16b-4f87-8c43-c7ad640f1d3b" />
    
- 표면적인 문법적인 차이를 빼고도 두 가지 방법은 실행 흐름이 크게 다르다. 대용량 파일을 분석해보면 차이가 드러난다.
- 같은 작업을 스크립트(예: Ruby/Python)로도 구현할 수 있으나, **파이프라인**은 조합과 재사용성이 높고, 대용량에서 성능 차가 드러난다.

### 2-3. 정렬 대 인메모리 집계

- **인메모리 해시 집계**: 키 종류가 적고 메모리에 전부 올라갈 때 유리.
- **외부 정렬-병합**: 데이터가 커서 메모리에 안 들어가면 유리(메모리 청크 정렬→임시 파일→다단계 병합). **순차 I/O**를 최대화한다.
    
    참고: sort 유틸리티는 입력이 크면 자동으로 임시 파일을 사용한다.
    
- 앞선 예제에서 루비 스크립트와 유닉스 파이프라인은 동작 방식이 약간 다르다.
    - 루비 스크립트: URL 해시 테이블을 메모리에 유지한다. 해시 테이블에 각 URL이 출현한 수를 매핑.
    - 유닉스 파이프라인: 해시 테이블 대신 정렬된 목록에서 같은 URL이 반복해서 등장.
- 어떤 접근법이 좋을까? → 다른 URL이 얼마나 되느냐가 중요.
- 작은 데이터셋 → 인메모리 해시 접근 방식
    - 데이터가 작고, 한번에 메모리에 모두 올라갈 수 있다면 **해시 테이블** 사용하는게 효율적.
    - 예를 들어, 수 백만 개의 로그가 모두 같은 URL을 가진다면 URL을 키로 하고 해당하는 카운트를 해시로 저장하면 된다.
    - 이 경우 디스크 접근 없이 메모리에서만 처리되므로 매우 빠르며, 작업 세트(working set)가 충분히 작을때 이상적이다.
- 큰 데이터셋 → 정렬 기반 접근 방식
    - 데이터가 너무 커서 메모리에 모두 담기지 않는다면 **정렬 후 병합(sort-merge)** 접근법이 더 적절하다.
    - 원리:
        1. 메모리에 데이터를 일정 크기로 모은다.
        2. 메모리 내에서 정렬한다.
        3. 정렬된 청크(chunk)를 디스크에 저장한다.
        4. 이후 여러 개의 정렬된 청크 파일을 한꺼번에 병합 정렬(merge sort) 하여 큰 정렬 파일을 만든다.
    - 이렇게 하면 **순차적 디스크 접근(sequential I/O)** 이 가능해져 디스크 효율이 높아진다.
- 리눅스의 sort유틸리티로 같은 원리를 사용한다. 입력 데이터가 너무 크면 자동으로 여러 임시 파일을 디스크에 쓰고, 나중에 병합 정렬을 수행한다.

### 2-4. 유닉스 철학

- **한 가지 일**만 잘 하는 작은 도구를 만들고, **표준 입출력**으로 **연결**해 조합한다.
- 핵심 원칙: 책임 분리, 출력→입력 호환, 빠른 실험/재작성.
- 앞의 예제와 같이 연쇄 명령을 통해 쉽게 로그 파일을 분석할 수 있었던 것은 유닉스 핵심 설계 아이디어 중 하나이다.
- “데이터 처리 과정에서 서로 다른 프로그램을 정원 호스처럼 연결할 수 있는 방법이 필요하다.” - 더그 맥일로이
- 즉, 유닉스는 하나의 프로그램이 단일 작업을 수행하고, 그 결과를 다른 프로그램의 입력으로 연결할 수 있도록 설계 되었다. 이 단순한 개념이 유닉스 철학의 뿌리가 되었다.
- 오늘날에도 DevOps, 마이크로서비스, 데이터 파이프라인 설계의 기본 원리로 이어지고 있다.

| **번호** | **원칙** | **의미** |
| --- | --- | --- |
| **1** | 각 프로그램은 한 가지 일만 잘 하도록 작성하라. | 하나의 프로그램이 여러 기능을 하지 말고, 명확한 책임을 가져야 한다. |
| **2** | 프로그램의 출력은 다른 프로그램의 입력이 될 수 있게 하라. | 표준 입력/출력 인터페이스를 유지해, 조합 가능성을 높인다. |
| **3** | 소프트웨어를 빠르게 만들고 버릴 수 있게 하라. | 완벽주의보다 실험과 반복, 재구축이 더 중요하다. |
| **4** | 도구를 만드는 데 시간을 쓰지 말고, 도구를 활용해 목표를 달성하라. | 개발 편의성보다 생산성을 우선하라. |

### 2-5. 동일 인터페이스

- 파이프라인의 전제는 **호환 포맷**이다. 유닉스의 공통 인터페이스는 **파일(바이트 스트림)** 로, 서로 다른 도구를 쉽게 연결한다.
- 어떤 프로그램의 **출력을 다른 프로그램의 입력으로 연결**하려면 두 프로그램이 **같은 형식의 인터페이스**를 사용해야 한다.
- 즉, **“호환 가능한 입출력 포맷”** 이 있어야 파이프라인이 가능하다.
    - 예를 들어 한 프로그램의 출력이 JSON인데, 다음 프로그램이 CSV만 이해한다면 연결이 불가능하다.
- 유닉스의 기본 인터페이스는 파일(file)이며, 파일은 단순히 정렬된 바이트의 연속이므로, 단순한 구조 덕분에 다양한 대상을 같은 방식으로 다룰 수 있다.
- 즉, 유닉스에서는 “모든 것을 파일로 다룬다”는 철학이 존재하고, 이 때문에 서로 다른 역할을 하는 프로그램들도 동일한 인터페이스를 통해 쉽게 연결할 수 있다.

### 2-6. 로직과 연결의 분리

- 유닉스의 표준 입력(stdin)과 표준 출력(stdout)은 다양할 수 있다. 키보드와 화면에서부터 파일 등 다양하게 지원한다.
- 파이프는 한 프로세스의 stdout을 다른 프로세스의 stdin과 연결한다. 이 때 중간 데이터를 디스크에 쓰지 않고 작은 인메모리 버퍼를 사용해 프로세스 간 데이터를 전송한다.

```bash
cat input.txt | sort | uniq > output.txt
```

- 도구는 **계산**만, **연결**은 쉘/파이프가 맡는다.
    
    → **느슨한 결합**, **지연 바인딩**, **제어 반전**이 자연스럽게 달성된다.
    
- 이 구조의 핵심은 프로그램들이 **서로의 존재나 위치를 몰라도 연결이 가능하다는 점**이다.
- 즉, 프로그램 A는 “내 출력이 더리도 가는지” 신경 쓸 필요가 없고, 프로그램 B도 “내 입력이 어디서 오는지” 몰라도 된다.
- 이런 형태를 다음과 같이 표현한다.
    - 느슨한 결합 (loose coupling): 서로의 구체적인 구현에 의존하지 않는 연결 구조
    - 지연 바인딩 (late binding): 런타임에 연결 관계가 결정됨 (미리 하드코딩하지 않음)
    - 제어 반전 (inversion of control): 데이터의 흐름이 프로그램 내부 로직이 아니라 외부(파이프라인)에 의해 결졍됨
- 이러한 구조 덕분에 프로그램은 **입출력 연결**과 **내부 로직**을 **분리**할 수 있다.
- 설계상의 장점
    - 조합 가능성 증가
    - 테스트와 유지보수 용이
    - 재사용성 극대화

### 2-7. 투명성과 실험

- 입력 **불변성**(원본 미변경), **중간 결과 관찰**, **부분 재실행**이 쉬워 디버깅·실험에 적합하다.
- 유닉스 도구가 성공할 수 있었던 이유 중 하나는 투명성이다. 즉, 사용자가 프로그램의 진행 상황과 중간 결과를 쉽게 관찰하고 실험할 수 있었기 때문이다.
- 다음과 같은 특징이 있다:
    - 유닉스 명령어는 입력 파일을 절대 변경하지 않는다. (불변성) → 원본 데이터 손상될 걱정 없이 실험 반복 가능
    - 중간 결과 관찰 가능. → 디버깅 가능
    - 부분 재실행. → 파이프라인 중간 결과를 파일에 저장했다가 나중에 이어서 실행할 수 있다. → 전체 파이프라인을 처음부터 실행하지 않아도 됨.

## 3. 맵리듀스와 분산 파일 시스템

- **MapReduce**는 유닉스 파이프라인의 아이디어를 **수천 대**로 확장한다. 하나의 작업이 하나 이상의 입력을 읽어 하나 이상의 출력(디렉터리)을 만든다.
- 맵리듀스는 유닉스 도구와 비슷하지만, 수천 대의 장비로 분산해서 실행이 가능하다.
- 단일 맵리듀스 작업은 하나 이상의 입력을 받아 하나 이상의 출력을 만들어낸다.

📌 HDFS(Hadoop Distributed File System) 이란?

- 파일을 **블록**으로 쪼개 여러 **데이터노드**에 저장하고, **복제**(또는 **삭제코딩(Erasure Coding)**)으로 내결함성을 확보한다.
- 대규모 데이터를 **여러 장비에 분산 저장**하고, 병렬로 읽고 쓸 수 있는 시스템
- **네임노드(NameNode):**
    - 중앙에서 **메타데이터**를 관리하는 서버
    - 어떤 파일의 블록이 어떤 장비(Node)에 저장되어 있는지 추적
    - 일반 데이터 블록은 분산되어 있지만, 네임노드는 전체 파일 시스템의 “지도 역할”을 한다.
    - 즉, 네임노드는 파일의 위치 정보만 관리하고, 실제 데이터는 여러 장비(데이터노드)에 저장된다.
- **데이터노드(DataNode)**
    - 실제 파일의 블록을 저장하고, 클라이언트의 읽기/쓰기 요청을 직접 처리한다.
    - 여러 노드가 동시에 협력하여 데이터를 읽거나 쓸 수 있다.
- HDFS의 복제(Replication) 및 내결함성(Fault Tolerance)
    - 대규모 분산 시스템에서는 장비나 디스크 고장이 필연적으로 발생한다.
    - HDFS는 기본적으로 하나의 파일 블록을 여러 노드에 **복제본(Replica)** 으로 저장
    - 단순 복제 대신 수학적으로 복구 가능한 코딩 방식(리드 솔로몬 코드 등)을 사용
    - RAID(디스크 복제 기술)와 유사하지만, 특별한 하드웨어 없이 소프트웨어적으로 분산 환경에서 구현함.

| **구분** | **NAS / SAN** | **HDFS** |
| --- | --- | --- |
| **구조** | 중앙 집중형 스토리지 | 노드 로컬 디스크 분산 저장 |
| **접근 방식** | 하드웨어 기반 공유 디스크 | 소프트웨어 기반 네트워크 접근 |
| **확장성** | 고가의 전용 하드웨어 필요 | 일반 서버 수천 대로 확장 가능 |
| **복구 방식** | RAID 등 장비 수준 | 데이터 블록 단위 복제 / 삭제코딩 |
- 즉, NAS나 SAN은 **비용이 많이 들고 확장성이 떨어지지만**, HDFS는 **일반 장비를 이용해 저비용으로 대규모 데이터 처리**가 가능하다.
- 네임노드와 데이터노드 구조 덕분에 **수평 확장(horizontal scaling)** 이 자연스럽다.
    
    즉, 서버를 추가하기만 하면 저장용량과 처리량이 함께 증가한다.
    

### 3-1. 맵리듀스 작업 실행하기

- 맵리듀스는 기본적으로 입력 데이터 → Mapper → Sort → Reducer → 출력의 구조로 동작한다.
- 유닉스 파이프라인의 각 명령을 클러스터 전체에 분산시켜 실행할 수 있게 만들어 준다.
1. **입력 단계 (Input):** HDFS의 파일들을 **스플릿**으로 분할(로그 한 줄=레코드).
    - 대규모 입력 데이터를 레코드(record) 단위로 분할한다.
    - 예를 들어, 웹 서버 로그 파일이라면 각 줄(\n으로 구분된 한 줄)이 하나의 레코드가 된다.
    - 이 입력은 **HDFS 블록 단위로 나뉘어 여러 노드에 분산**되어 저장되어 있다.
2. **맵 단계(Map):  레코드 → emit(key, value)(예: (URL, 1)).**
    - 각 입력 레코드에 대해 **매퍼 함수(Map Function)** 가 호출된다.
    - 매퍼는 입력에서 **키(key)** 와 **값(value)** 을 추출한다.
        - 예: awk '{print $7}'처럼 URL 필드를 추출 → (URL, 1) 형태로 변환
    - 이때 생성된 (key, value) 쌍은 임시 버퍼에 저장된다.
3. **셔플(Sort & Shuffle): 키 기준 정렬·파티셔닝으로 같은 키를 모은다.**
    - 모든 (key, value) 쌍을 **키 기준으로 정렬(Sort)** 한다.
    - 같은 키를 가진 값들은 **묶여서 하나의 리스트**로 만들어진다.
4. **리듀스 단계 (Reduce): key, [values]를 집계(예: 합계)해 출력.**
    - 같은 키 그룹별로 **리듀서 함수(Reduce Function)** 가 호출된다.
    - 리듀서는 해당 키에 속한 모든 값들을 **집계(Aggregation)** 하거나 **통계 처리**를 수행한다.
        - 예: 같은 URL이 여러 번 등장했다면 이를 모두 더해 “URL별 방문 횟수” 계산

### 3-2. 맵리듀스의 분산 실행

- 맵리듀스 프레임워크에서 사용자는 병렬 코드를 직접 쓰지 않는다. 프레임워크가 클러스터 전체로 배치, 실행, 재시도, 정렬, 전송을 자동 수행한다.
- 데이터 근처에서 실행: 가능하면 입력 블록이 저장된 노드에서 맵 태스크를 실행한다 (데이터 로컬리티). 이렇게 하면 네트워크 비용을 줄일 수 있다.

<img width="576" height="405" alt="image" src="https://github.com/user-attachments/assets/b7c1ff4b-cb00-4f6b-a1ae-47cb675bf11e" />

**1. 입력 파티셔닝과 맵 태스크 배치**

- HDFS 같은 분산 파일 시스템에서 **입력 디렉터리**를 지정하면, 각 파일이 **블록/스플릿(splits)** 으로 잘려 여러 노드에 분산 저장됨
- **각 스플릿마다 1개의 맵 태스크**가 생긴다. (그림 10-1에 m1, m2, m3…로 표기).
- 스케줄러는 그 스플릿을 갖고 있는 노드의 **CPU/메모리 여유**를 보고 그 노드에서 태스크를 실행하려고 시도(데이터 가까이에서 연산하기 → 네트워크 부담 ↓ 로컬리티 최적화 ↑)

**2. 맵 단계(Map)**

- 맵 함수는 각 레코드를 받아 (key, value) 쌍을 방출한다. 예를 들어, 웹 로그면 (URL, 1) 방출
- 메모리 버퍼가 차면 중간 결과를 **디스크로 “스필(spill)”** 하며, 스필 파일은 **키 기준 정렬**되어 저장된다.여기서 사용하는 외부정렬/병합 원리는 책의 SSTable/LSM 트리 절과 동일한 이유로 선택됨: 큰 데이터의 **순차 I/O**를 위해

**3. 셔플(shuffle)과 머지(merge)**

- 리듀서가 시작되면 각 맵 태스크의 중간 결과를 **네트워크로 끌어와(pull)** 자기 로컬 디스크에 받는다.
    
    이때 해시 파티셔너가 “키 → 어떤 리듀서”로 갈지 결정(일반적으로 hash(key) % R)
    
- 리듀서는 받은 여러 스필 조각을 **다단계 병합 정렬**로 하나의 큰 정렬 스트림으로 만든다.
    
    → 결과적으로 **“같은 키의 값들이 인접”** 한 정렬된 데이터 스트림이 리듀서에 도착
    

**4. 리듀스 단계(Reduce)**

- 리듀스 함수는 **같은 키에 대한 값들의 이터레이터**를 받아 집계/요약한다. (예: 합계)
- 출력은 다시 HDFS에 **순차적으로 기록**된다. (쓰기-한-번, 읽기-여러-번 모델)

> 맵은 “데이터 로컬리티” 덕을 많이 보지만, **리듀스의 셔플은 필연적으로 네트워크 I/O**가 크다. 그래서 키 분포가 한쪽으로 쏠리면(핫 키) 특정 리듀서가 병목이 된다. → **키 파티셔닝/리듀서 수/스큐 완화**가 중요.
> 

### 3-3. 맵리듀스 워크플로

- 맵리듀스는 “URL당 페이지뷰 수 세기” 등의 단순한 작업은 잘 처리하지만, “가장 인기 있는 URL 찾기”와 같이 한 번의 작업으로는 복잡한 분석을 수행할 수 없다.
- Becuase, 맵리듀스는 하나의 입출력 흐름을 가지므로, 추가적인 정렬이나 집계가 필요하기 때문이다.
- 따라서, 여러 개의 맵리듀스 잡을 연결해 처리하는 방식인 워크플로(workflow)가 필요하다.
- 맵리듀스에서는 한 작업의 **출력 디렉터리(Output Directory)** 를 다음 작업의 **입력 디렉터리(Input Directory)** 로 사용한다.
- 예시:
    - 1단계: URL별 페이지뷰 수 세기 → HDFS에 결과 저장
    - 2단계: 결과 파일에서 상위 10개 URL 추출
- 이렇게 여러 단계의 작업을 연결하여 **복잡한 데이터 파이프라인**을 구성할 수 있다.
- 하둡의 맵리듀스 프레임워크는 워크플로 기능을 직접 제공하지 않기 때문에 아래와 같은 다양한 도구를 이용한다.

| **분류** | **도구명** | **특징** |
| --- | --- | --- |
| 워크플로 스케줄러 | **Oozie**, **Azkaban**, **Luigi**, **Airflow**, **Pinball** | 잡 실행 순서·의존성 관리, 실패 시 재시도 |
| 데이터 파이프라인 프레임워크 | **Pig**, **Hive**, **Cascading**, **Crunch**, **Flume** | SQL 스타일로 맵리듀스를 자동 생성·연결 |

### 3-4. 리듀스 사이드 조인과 그룹화

- 조인은 여러 데이터셋을 연결하는 연산으로, DB에서는 인덱스로 필요한 데이터만 읽지만 맵리듀스는 인덱스가 없어 **전체 데이터를 병렬로 스캔**한다.
- 작은 범위 질의는 DB가, 대규모 배치 분석은 맵리듀스가 효율적이다.

### 3-5. 사용자 활동 이벤트 분석 예제

<img width="548" height="274" alt="image" src="https://github.com/user-attachments/assets/3d141855-f044-4fd8-838d-a390619a2021" />

- 목표: 활동 로그와 사용자 프로필을 **조인(join)** 해서 분석한다. (예: 나이대별 인기 페이지 등)
- 한계: 이벤트 로그를 읽으며 user_id마다 **데이터베이스에 쿼리**를 보내는 방식은 구현은 간단하지만 **매우 비효율적**이다.
- 해결: **데이터베이스의 스냅샷(사본)** 을  **HDFS에 저장하고,** 맵리듀스에서 **이벤트 로그 + 사용자 데이터 사본**을 함께 읽어 로컬에서 병렬로 조인을 수행한다. → 네트워크 I/O 감소, 고속 조인 가능

### 3-6. 정렬 병합 조인

- **정렬 병합 조인(Sort-Merge Join):** 매퍼가 키 기준으로 데이터를 정렬해 리듀서로 보내면, 리듀서가 같은 키의 데이터를 병합해 한 번에 처리한다.
- **매퍼(mapper)** 는 입력 데이터를 읽고 (키, 값) 쌍을 추출한다.
    - 이벤트 로그 매퍼: (user_id, 이벤트)
    - 사용자 DB 매퍼: (user_id, 생일)
- **리듀서(reducer)** 는 같은 키(여기선 user_id)를 가진 값들을 모아 하나로 처리한다.

<img width="731" height="302" alt="image" src="https://github.com/user-attachments/assets/7da36104-6146-4ce9-87e7-73a016d6b42e" />

1. **모든 매퍼 출력**은 user_id 기준으로 정렬 후 파티셔닝된다.
2. 같은 사용자 ID를 가진 데이터가 동일한 리듀서로 전달됨.
3. 리듀서는 각 사용자 ID에 대해 한번만 호출된다.
    - 첫 번째로 사용자 정보(생년월일 등)를 읽고, 이어서 해당 사용자의 이벤트들을 시간순으로 처리하게 할 수 있는데, 이를 **보조 정렬(secondary sort)** 이라고 한다. (맵리듀스에서 작업 레코드를 재배열 한다.)
4. 최종적으로 URL(본 항목)과 연령(viewer-age) 쌍을 만들어 연령대별 분석이나 분류에 활용할 수 있다.

### 3-7. 같은 곳으로 연관된 데이터 가져오기

- 맵리듀스는 키를 목적지 주소처럼 사용해 관련 데이터를 한 리듀서로 모으고, 리듀서는 같은 키에 해당하는 모든 레코드를 **한 번에 처리**하므로 메모리 효율이 높고, 단순한 단일 스레드 코드로도 충분히 동작한다.
- 맵리듀스는 이런 식으로 애플리케이션 로직과 **데이터 이동(통신)** 을 완전히 분리한다.
    
    → DB처럼 애플리케이션 코드에서 데이터를 직접 가져오지 않아도 됨.
    

### 3-8. 그룹화

- **그룹화(Grouping)** 는 SQL의 GROUP BY와 같은 개념으로, 같은 키를 가진 레코드들을 묶어 집계(aggregation)를 수행하는 것.
- 예시:
    - 페이지뷰 수 세기 → COUNT(*)
    - 특정 필드 합산 → SUM(fieldname)
    - 상위 k개의 항목 선택 → 랭킹 함수
- 매퍼(mapper)가 (키, 값) 쌍을 생성하면, **같은 키를 가진 값들은 리듀서(reducer)** 로 자동으로 모인다.
- 즉, 맵리듀스의 그룹화는 조인과 유사한 방식으로, **파티셔닝 → 정렬 → 리듀스** 단계로 이루어진다.
- **세션화(Sessionization):** 사용자의 여러 이벤트 로그를 **사용자별로 묶어 세션 단위로 분석**하는 과정.
    - 예: A/B 테스트, 마케팅 캠페인 분석, 사용자 행동 추적 등.
    - 동일한 사용자 ID로 묶기 때문에 여러 서버에 분산된 이벤트도 리듀서에서 한데 모아 분석할 수 있다.

### 3-9. 쏠림 다루기

- 특정 키에 너무 많은 데이터가 몰리면, 그 키를 담당한 리듀서(reducer)만 과부하가 걸려 작업이 늦어진다.
- 이런 불균형을 만드는 키(예: 유명 인물, 인기 태그 등)를 **핫 키(hot key)** 또는 **린치핀 객체(linchpin object)** 라고 부른다.
- 전체 작업은 모든 리듀서가 끝나야 완료되므로 한 리듀서가 느리면 전체가 지연된다.
- 해결 방법들:
    
    **1. 쏠린 조인(Skewed Join, Pig)**
    
    - 샘플링으로 핫 키를 미리 감지하고, 해당 키의 데이터를 여러 리듀서에 **분산 복제**시켜 병렬 처리.
    - 약간의 중복이 생기지만 전체 효율은 크게 향상됨.
    
    **2. 공유 조인(Shared Join, Crunch)**
    
    - 샘플링 작업 대신 핫 키를 미리 **명시적 지정**.
    - 파티셔닝된 DB와 유사한 방식으로 랜덤 분산시킴.
    
    **3. 하이브(Hive)의 방법**
    
    - 핫 키를 **메타데이터에서 미리 지정**하고, 관련 데이터를 별도 파일에 저장해 나중에 병합할 때 **맵 사이드 조인(Map-Side Join)** 으로 처리.

### 3-10. 맵 사이드 조인

- 맵  **사이드 조인(map-side join)**: 데이터가 이미 정렬·파티셔닝되어 있다면 리듀서 없이 **매퍼 단계**에서 조인 → 빠르고 효율적
- **브로드캐스트 해시 조인 (Broadcast Hash Join)**
    - 한쪽 데이터셋이 작을 때 사용.
    - 작은 테이블 전체를 각 매퍼 메모리에 로드해 큰 테이블과 직접 조인.
    - 네트워크 통신 거의 없음 → 빠르지만 작은 데이터셋만 가능.
- **파티션 해시 조인 (Partitioned Hash Join)**
    - 두 테이블을 같은 기준으로 **파티셔닝**해서 각 매퍼가 해당 파티션만 읽어 조인.
    - 대용량 데이터에도 적합하지만 사전 파티셔닝이 필요.
    - Hive에서는 이를 **버킷 조인(bucketed map join)** 이라 부름.
- **맵 사이드 병합 조인 (Map-Side Merge Join)**
    - 입력이 이미 정렬되어 있다면 매퍼가 **병합 정렬 방식**으로 조인.
    - 별도의 해시 테이블 불필요 → 가장 효율적이지만 정렬 상태 유지가 전제.

### 3-11. 맵 사이드 조인을 사용하는 맵리듀스 워크플로

- **리듀스 사이드 조인**은 키를 기준으로 정렬 후 리듀서에서 수행하지만, **맵 사이드 조인**은 입력 데이터가 이미 파티셔닝, 정렬되어 있으면 **매퍼에서 직접 조인** 가능.
- 브로드캐스트 조인을 사용할 경우 각 매퍼가 전체 작은 입력을 메모리에 올려 빠르게 처리.
- 최적화를 위해서는 데이터의 **물리적 배치(파티션 수, 정렬 기준)** 를 이해해야 함.
- Hadoop 생태계에서는 이런 메타데이터를 **Hive 메타스토어**나 **HCatalog**에서 관리.

### 3-12. 일괄 처리 워크플로의 출력

- 맵리듀스는 여러 단계를 연결해 하나의 **워크플로우**를 구성하고, 분석 보고서나 통계 지표 등 **일괄 처리 결과물**을 생성.
- OLTP(트랜잭션 처리)는 소량 데이터에 즉각 응답하지만, 맵리듀스는 대량 데이터를 집계·분석해 **시간 경과에 따른 통계나 변화 추이**를 보여줌.

### 3-13. 검색 색인 구축

- 구글은 검색 엔진 색인을 구축하기 위해 **5~10단계의 맵리듀스 워크플로우**를 사용했다.
- 검색 엔진의 색인은 특정 키워드를 입력하면 그 키워드를 포함한 **문서 ID 목록**을 효율적으로 찾는 구조.
    
    이는 일종의 **역색인(inverted index)** 으로, ‘단어 → 문서 목록’ 형태의 매핑을 생성한다.
    
    단순히 한 번 색인을 만드는 것뿐 아니라, **관련성(relevance)** 점수나 **오타 수정** 등 부가 정보를 함께 다루기도 한다.
    
- 맵리듀스는 문서 집합을 파티셔닝하고 각 리듀서가 해당 파티션의 색인을 구축한다.
- 완성된 색인은 분산 파일 시스템(HDFS)에 저장되며, **병렬 분산 처리**로 매우 효율적으로 수행됨.
- 문서가 변경될 때는 전체 색인을 새로 만드는 대신, 변경된 부분만 추적하여 **증분 색인(incremental indexing)** 으로 업데이트할 수도 있다.

### 3-14. 일괄 처리의 출력으로 키-값을 저장

- 검색 색인처럼, 일괄 처리(batch) 작업의 결과는 **데이터베이스(DB)** 형태로 저장되어 이후 웹 애플리케이션 등에서 **조회용**으로 사용된다.
- 예시:
    - 추천 시스템에서 사용자의 ID를 키로 관련 상품 목록을 저장
    - 검색 엔진의 색인 테이블
    - 머신러닝 피처 저장소 등
- But, 매퍼와 리듀서 내에서 DB를 직접 호출하게 되면 여러 문제가 발생한다.
    - 네트워크 요청이 많아져 **속도 저하**
    - 수천 개의 매퍼·리듀서가 동시에 DB에 접근 → **부하 과중 및 장애 위험**
    - 실패 시 재시도(task retry, speculative execution) 과정에서 **중복 저장 가능성**
- 따라서, 맵리듀스의 결과를 DB에 바로 쓰지 않고, **HDFS 상에 데이터베이스 포맷 파일**(예: HFile, SSTable 등)로 저장함.
- 이 HDFS 출력 파일을 나중에 **DB 서버로 한 번에 이관(bulk load)** 하는 별도의 단계에서 반영함.
- 즉, DB는 네트워크를 통한 실시간 요청을 처리하지 않고 이미 정렬된 완성 파일을 읽어들이는 형태로 병합.
- 정리하자면, 맵리듀스는 개별 insert 대신, **HDFS에 DB형식 파일을 만들어** 나중에 **DB 서버에 통째로 반영(벌크 로딩)**

### 3-15. 일괄 처리 출력에 관한 철학

- 리듀스는 **입력 불변성과 재실행 가능성**을 중시하는 유닉스 철학을 따른다.
- 실패 시에도 입력 상태를 그대로 두고 다시 실행하면 동일 결과를 보장한다.
- 이러한 구조는 복잡한 트랜잭션 관리가 필요 없고, **유지보수성과 안정성**이 뛰어난 일괄 처리 시스템을 만든다.
- 핵심 철학
    - 입력은 불변, 출력은 새로 생성.
    - 실패해도 동일 입력으로 언제든 다시 계산 가능.
    - 상태를 남기지 않으므로 유지보수와 오류 복구가 용이하다.

### 3-16. 하둡과 분산 데이터베이스의 비교

- 하둡은 **유닉스 철학 기반의 분산 시스템**으로, HDFS(파일 시스템)와 맵리듀스(데이터 처리 프레임워크)로 구성되어 있다.
- 맵리듀스의 핵심 개념은 완전히 새로운 것이 아니라, 기존 **대규모 병렬 처리(MPP: Massively Parallel Processing)** 데이터베이스 시스템(예: Teradata, Tandem NonStop SQL 등)의 확장된 형태다.
- 그러나, **MPP DB는 장비 클러스터에서 분석 SQL 질의를 병렬 수행하는 것에 중심**을 두지만, **하둡은 분산 파일 시스템을 기반으로 어떤 프로그램이든 실행할 수 있는 개방형 구조**를 제공한다.

### 3-17. 저장소의 다양성

- **전통적인 데이터베이스(DB)** 는 관계형·문서형 등 특정 **모델에 따라 데이터를 구조화**해야 한다.
- 반면 **하둡(HDFS)** 은 어떤 형태의 데이터든 그대로 저장 가능하다. (텍스트, 이미지, 비디오, 센서 데이터 등)
    
    → 데이터를 미리 가공하지 않고 **원시(raw) 형태로 덤프(dump)** 해두고, 필요할 때 해석한다.
    
- 이는 **데이터 웨어하우스(Data Warehouse)** 개념과 유사하지만, 하둡은 데이터를 모아두기만 하고 나중에 스키마를 적용하는 **스키마 온 리드(schema-on-read)** 방식을 사용한다.
- 이런 접근은 데이터를 저장할 때 형식 제약이 없고, 나중에 다양한 방식으로 해석·활용할 수 있는 유연성을 제공한다. 즉, “**원시 데이터가 더 좋다(Sushi Principle)**”는 철학에 기반한다.

### 3-18. 처리 모델의 다양성

- **MPP 데이터베이스**
    - SQL 기반으로 설계된 일체형(monolithic) 구조
    - 질의 계획, 스케줄링, 실행 등이 하나로 통합되어 특정 쿼리 유형에서는 매우 빠르다.
    - Tableau 같은 BI 도구와 연동이 쉽다.
    - 하지만 **SQL만으로는 머신러닝, 추천 시스템, 이미지 분석** 등 복잡한 처리를 표현하기 어렵다.
- **맵리듀스**
    - SQL 외의 복잡한 연산을 **엔지니어가 직접 코드로 작성 가능**
    - HDFS 기반에서 다양한 형태의 일괄 처리(batch processing)에 적합하다.
    - 다만 반복적 질의나 실시간 처리는 비효율적이라는 한계가 존재한다.
- **하둡 이후의 확장**
    - 이러한 한계를 보완하기 위해 **SQL 기반과 코드 기반을 함께 지원하는 다양한 처리 모델**이 등장
    - 하이브(Hive), 임팔라(Impala) 등은 SQL 질의 엔진을 제공하며, 하둡 위에서 SQL과 코드형 분석을 모두 수행할 수 있게 한다.
- **통합 생태계**
    - 하둡 생태계는 **HBase(OLTP용)** 와 **Impala(MPP 분석용)** 같은 서로 다른 접근 방식을 모두 포함
    - 두 시스템 모두 HDFS를 저장소로 사용하며, 접근 방식은 다르지만 **공존하거나 통합 운영**이 가능하다.
- 즉, 하둡 생태계는 SQL 기반 MPP 분석과 코드 기반 일괄 처리를 모두 포괄하며, HBase, Impala 같은 시스템이 HDFS 위에서 **다양한 처리 모델을 공존·통합**시키는 방향으로 발전했다.

### 3-19. 빈번하게 발생하는 결함을 줄이는 설계

- **MPP 데이터베이스**
    - 질의 실행 도중 장애가 발생하면 전체 질의가 중단된다.
    - 사용자가 재시도하거나 시스템이 자동으로 재실행해야 한다.
    - 대부분의 질의가 짧게 끝나므로 이 방식의 재시도 비용은 크지 않다.
    - 메모리에 많은 데이터를 유지하여 디스크 접근을 최소화한다.
- **맵리듀스**
    - 개별 **맵 또는 리듀스 태스크 단위로 실패를 복구**할 수 있다.
    - 전체 작업이 아니라 실패한 태스크만 재실행하므로 영향이 적다.
    - 데이터는 항상 디스크(HDFS)에 기록되어 복구가 쉽다.
    - 메모리에 데이터를 올려두기엔 너무 크기 때문에 디스크 기반 접근이 일반적이다.
- 장시간·대규모 데이터 처리 시 일부 태스크 실패는 흔하므로 태스크 단위 복구는 매우 실용적이다.
- 구글의 **클러스터 스케줄러**는 태스크마다 **우선순위(priority)** 를 부여한다.
    - 높은 우선순위 작업에는 더 많은 리소스(CPU, RAM, 디스크 등)를 배정한다.
    - 낮은 우선순위 작업은 리소스가 부족할 때 중단(preempt)되기도 한다.
    - 우선순위가 높을수록 처리 속도와 비용도 상승.
- **선점(preemption) 기반 스케줄링**
    - 낮은 우선순위 태스크의 자원을 회수해 높은 우선순위 태스크에 할당한다.
    - 하둡에서는 이런 선점 설계를 통해 클러스터 자원 활용률을 높인다.
    - 태스크 실패는 예외적 상황이 아니라 **자원 최적화 전략의 일부**로 설계되었다. (선점=태스크 종료)
    - 예: 구글에서는 태스크 중 5% 이상이 선점(preempt)되더라도 전체 성능에 큰 영향 없음.

## 4. 맵리듀스를 넘어

- 2000년대 후반 맵리듀스는 크게 유행했지만, 분산 시스템에서 가능한 많은 프로그래밍 모델 중 하나일 뿐이며 데이터/구조/처리 방식에 따라 더 적합한 도구가 따로 있을 수 있다.
- 맵리듀스가 널리 쓰인 이유는 **분산 파일 시스템 위의 대규모 배치 처리에 명확한 모델**을 주기 때문. “단순함”은 쓰기 쉽다는 뜻이 아니라, **무엇을 하는지 이해하기 쉬운 모델**이라는 뜻이다.
- 원시 API로 복잡한 연산을 직접 짜기는 어렵다. 그래서 **Pig, Hive, Cascading, Crunch** 같은 고수준 모델이 등장해 맵리듀스 실행을 추상화하고 일괄 처리를 쉽게 만들어준다.
- 다만 **실행 모델 자체의 한계**가 있다. 반복/점증/상호작용 작업 등 추상화만으로 해결되지 않는 부류에서는 성능/지연이 나빠지기 쉽다.
- 반대로 맵리듀스의 강점은 **견고함과 대규모 확장성**. 불안정한 멀티 테넌트 클러스터에서도 대량 데이터를 신뢰성 있게 처리하는 배치 작업에는 매우 성공적이다.
- 결론: 맵리듀스가 만능은 아니다. **작업 유형에 따라 더 적합한 도구**를 고르는 것이 좋다. 이어지는 장들(특히 11장)은 일괄 처리의 다른 대안인 **스트림 처리** 등으로 논의가 확장된다.

### 4-1. 중간 상태 구체화

- **잡 연결 방식**: `잡1 출력 디렉터리 → 잡2 입력 디렉터리`. 스케줄러가 **잡1 완료 후** 잡2 시작.
- **중간 상태(Intermediate state)**: 잡과 잡 사이에 생기는 **중간 결과 파일**.
- **구체화(Materialization)**: 중간 상태를 **분산 파일 시스템(HDFS/S3)** 에 **파일로 저장**해 두는 것.
- **대비 개념**: 유닉스 파이프는 **스트리밍**(중간 파일 없음), 맵리듀스는 **완전 구체화**가 기본.
- 맵리듀스는 “단계 사이 파일로 끊어 연결”한다. 중간 상태 구체화는 느리고 비싸지만 **내결함성·운영 편의·재사용성**이 커진다.

> **구체화**: 중간 결과를 파일로 “물건처럼” 남김
> 
> **스트리밍**: 앞단 출력이 버퍼를 타고 즉시 뒤단 입력으로 흘러감
> 

### 4-2. 데이터플로 엔진

- 맵리듀스의 한계를 보완하려고 **분산 일괄 처리 연산**을 수행하는 새로운 엔진들이 나왔다. 대표적으로 **스파크(Spark), 테즈(Tez), 플링크(Flink)**.
- 설계 차이는 있으나 **전체 워크플로를 독립된 하위 작업으로 쪼개지 않고 “작업 하나”로 다룬다**는 공통점이 있다.
- 이런 시스템을 **데이터플로 엔진**이라 부르며, **여러 처리 단계를 거치는 데이터 흐름을 명시적으로 모델링**한다.
    - 한 단계에서는 맵리듀스처럼 **사용자 정의 함수를 반복 호출**해 **레코드 단위로 처리**하고,
    - **입력을 파티셔닝**해 병렬화하며,
    - **한 함수의 출력을 다음 함수의 입력으로** 넘기기 위해 **네트워크로 복사**한다.
- 맵리듀스와 비교했을 때 장점
    - **값비싼 정렬은 “필요할 때만” 수행**(맵↔리듀스 사이마다 항상 정렬하는 기본 모델과 대비).
    - **불필요한 맵 태스크가 없다**: 매퍼가 하던 일부 작업을 **중간 선행 리듀스 연산자**로 통합 가능(데이터셋 파티셔닝이 바뀌지 않으면 가능).
    - **조인/데이터 의존을 명시적으로 선언**하므로 스케줄러가 **데이터를 소비하는 태스크를 생산 태스크와 같은 장비**에 배치해, **네트워크 복사 대신 공유 메모리 버퍼**로 교환 가능.
    - **중간 상태는 메모리/로컬 디스크에 기록**하는 것으로 충분한 경우가 많아, **HDFS에 중간 상태를 기록·복제**하는 것보다 **I/O가 훨씬 적다**. (맵리듀스는 매퍼 출력에만 이 최적화를 적용하지만, 데이터플로 엔진은 **모든 중간 상태로 일반화**.)
    - **다음 단계는 입력이 준비되는 즉시 시작**할 수 있어, **선행 단계 전체 완료를 기다릴 필요가 없다**.
    - **기존 JVM을 재활용**해 새 태스크마다 JVM을 띄우는 맵리듀스보다 **시작 부담이 작다**.

### 4-3. 내결함성

- 맵리듀스는 **모든 중간 상태를 HDFS에 구체화(materialize)** 하여 장애 시 같은 입력을 다시 읽고 재시작함으로써 내결합성을 확보한다.
- 스파크·플링크·테즈는 **HDFS에 중간 상태를 쓰지 않는** 대신, 각자 다른 방식으로 실패 후 복구를 지원한다.
- 데이터플로 엔진의 복구 방식
    - 공통 전제: 어떤 입력 파티션에 어떤 연산자(연산자 그래프)가 적용되었는지를 추적해야 한다.
    - **Spark:** RDD 계보(라인리지, lineage)를 기록해 작업 중 실패가 나면 **해당 구간만 재계산**한다.
    - **Flink:** 연산자(태스크) **상태를 체크포인트**로 남겨, 실패 시 **연산자 상태를 복구**하고 이어서 수행한다.
    - **Tez:** HDFS에 중간 산출물을 쓰지 않으므로, 장비 장애로 중간 상태가 사라질 수 있는 경우 **원천 데이터부터 다시 계산**(가능하면 선행 단계 없이 바로 원천을 사용).
- **재연산이 가능하려면 “결정성”이 핵심이다.**
    - 동일한 입력이 주어졌을 때 **항상 같은 출력**이 나와야 재연산으로 복구가 가능하다(결정적 연산).
    - 연산이 **비결정적**이면(동일 입력인데도 출력이 달라지면) 이전 결과를 정확히 되살리기 어렵다. 이 경우 보통은 이전 결과에 맞추기보다 **다운스트림을 새 데이터로 간주**하고 다시 처리한다.
- 비결정성을 만드는 대표적 원인
    - **탐색 순서가 보장되지 않는 결합/조인**(예: 테이블 원소 순회 순서에 의존).
    - **통계적·확률적 알고리즘의 임의성**(난수에 의존).
    - **시스템 시계**(현재 시각), **외부 데이터/출력**에 의존하는 연산.
    - 대책: 가능하면 **결정적으로 만들기**(예: 고정된 시드의 의사난수 사용, 외부 의존 제거/격리).
- 재연산, 중간 상태 구체화 선택에는 정답이 없기 때문에 상황에 따라 적절한 판단이 필요하다.

### 4-4. 구체화에 대한 논의

- 맵리듀스: 각 태스크(맵/리듀스)의 **출력을 임시 파일로 기록**(중간 결과를 단계마다 구체화).
- 데이터플로 엔진(특히 **Flink**): **유닉스 파이프라인**처럼 연산자 간에 **출력을 바로 다음 연산자의 입력으로 전파**(입력이 모두 끝날 때까지 기다리지 않음).
- 작업이 끝난 **최종 출력**은 다른 사용자가 찾아 쓰도록 **지속성 있는 저장소(분산 파일 시스템 등)** 에 기록.
- 데이터플로 엔진을 사용할 때 파일 시스템(HDFS)에 구체화되는 데이터셋은 보통 “입력 데이터”와 “최종 결과”이다.
    - **중간 결과는 파이프라인으로 전파**하고, 단계마다 파일로 떨어뜨리지는 않음.
- 맵리듀스는 **모든 중간 상태를 파일로 남기는 구조**였던 반면, 데이터플로 엔진은 **사용자가 중간 단계를 일일이 파일로 기록하도록 강제하지 않음**.
- 결과적으로 **불필요한 디스크 I/O가 줄고** 파이프라인으로 **처리 지연이 단축**된다.

### 4-5. 그래프와 반복 처리

- 일괄 처리 맥락에서도 그래프 분석이 중요하다. 대표적으로 **페이지랭크**는 웹페이지 간 링크를 이용해 인기도(순위)를 정하고, 검색 결과 정렬 등에 쓰인다.
- 스파크·플링크·테즈 같은 **데이터플로 엔진**은 작업을 **연산자(operators)** 들의 **비순환 방향 그래프(Directed Acyclic Graph)** 로 배치한다.  여기서의 ‘그래프’는 **연산자 사이로 데이터가 흐르는 실행 계획**을 뜻한다.
- 그래프 분석엔 “특정 조건을 만족할 때까지” 간선을 따라 **반복적으로 전파·집계**하는 패턴이 흔하다.
    
    예: 한 지역과 연결된 모든 지역의 목록을 완성할 때까지 확장하는 **이행적 폐쇄(transitive closure)**.
    
- MapReduce는 매 반복마다 **전체 입력을 다시 읽고**, 중간 결과를 **파일로 완전히 물리적 기록**해야 한다. 알고리즘은 일부만 바뀌어도 **항상 새로운 전체 출력**을 만들어 비효율이 크다. 따라서 “완료될 때까지 반복”이라는 개념을 일반적 MapReduce로 표현하면 **비용이 상당히 커진다**.
- 반면, 데이터플로 엔진은 연산자 DAG 기반으로 **연산자 출력→입력**을 직접 연결하고, 필요 시 **인메모리 전달·파이프라이닝/스트리밍**을 활용해 **불필요한 재입출력**을 줄인다. 그 결과 반복·수렴형 그래프 알고리즘을 **자연스럽고 효율적으로** 표현할 수 있다.

### 4-6. 프리글 처리 모델

- 일괄 그래프 처리를 최적화하는 대표 방식이 **BSP(Bulk Synchronous Parallel)** 모델이다. 오픈소스 구현으로는 **Apache Giraph**, **스파크 GraphX API**, **플링크 Gelly API** 등이 있고, 구글의 **Pregel**이 이 모델의 전형으로 널리 알려져 있다.
- MapReduce에서 맵퍼가 키를 기준으로 리듀서를 호출해 값들을 모아 전달하듯이, Pregel에서도 **정점(vertex)에게 메시지를 보낸다**. 메시지는 **그래프의 간선을 따라 이웃 정점으로 전달**된다.
- MapReduce와 달리 Pregel의 정점은 **메모리상 상태를 반복 사이에 유지**한다. 따라서 **새 메시지를 받은 정점만 작업**을 수행하고, 메시지가 없는 정점은 **그 단계에서 아무 일도 하지 않는다**(비활성)
- 즉, Pregel/BSP는 “**정점 상태를 보존**하면서 **메시지를 간선을 통해 동기식 단계로 전달**”하는 구조로, 반복·수렴형 그래프 계산을 MapReduce보다 **자연스럽고 효율적**으로 수행한다.

### 4-7. 내결함성(프리글 처리 모델)

- 정점(vertex)들이 서로 직접 질의하는 대신 메시지 전달로 통신한다. 메시지는 일괄 처리(batch)되므로 통신 중 대기 시간이 거의 없고, **대기(time)** 는 오직 각 반복(슈퍼스텝)과 그 다음 반복 사이에서만 발생한다.
- 앞선 반복에서 보낸 **모든 메시지가 다음 반복 시작 전에 도착**해야 하므로, 프리글은 다음 반복을 시작하기 전에 이전 반복을 반드시 끝낸다. 이 보장을 위해 **모든 메시지는 네트워크 상에서 복제(복사)** 된다.
- 네트워크에서 메시지가 **유실·중복·지연**되더라도, 프레임워크는 다음 반복에서 **목적지 정점이 해당 메시지를 정확히 한 번 처리**하도록 보장한다. 즉, 프로그래밍 모델을 단순화하기 위해 **프레임워크 차원에서 결함 복구를 담당**한다.
- 반복이 끝날 때 **모든 정점의 상태를 주기적으로 체크포인트**로 영속 저장소에 기록한다(상태 스냅샷).
- 노드 장애로 인메모리 상태가 사라지면 **가장 단순한 복구**는 **마지막 체크포인트로 되돌린 뒤 연산을 재시작**하는 것이다.
    
    알고리즘이 **결정적**이고 메시지를 **로그로 남겼다면**, 손실된 **파티션만 선택 복구**하는 방식도 가능하다(전체 롤백 없이 부분 복구).
    

### 4-8. 병렬 실행

- 정점은 자신이 어느 장비에서 실행되는지 몰라도 된다. 정점 ID로 메시지를 보내면 **프레임워크가 파티셔닝·라우팅**(어느 장비가 실행할지, 네트워크로 어디로 보낼지)을 맡는다.
- 프로그래밍 모델은 “한 번에 정점 하나”를 다루므로 프레임워크가 임의 방식으로 그래프를 파티셔닝할 수 있다. 이상적으로는 **자주 통신하는 정점들을 같은 장비에 배치**하는 것이 좋지만, 실제로는 최적화가 어려워 **정점 ID 기반의 단순 분할**을 쓰는 경우가 많아 **관련 정점의 군집화가 잘 안 된다**.
- 분산 그래프 알고리즘은 **장비 간 메시지 통신 오버헤드**가 크다. 심지어 **중간 상태(노드 간 전송된 메시지)의 양이 원본 그래프보다 커질** 수도 있어, 네트워크 비용 때문에 전체 알고리즘이 **심각하게 느려질 수 있다**.

### 4-9. 고수준 API와 언어

- **엔진 성숙**: 대규모 분산 일괄 처리가 안정화되며 초점이 모델·효율로 이동.
- **고수준 도구 부상**: 맵리듀스 직접 코딩의 어려움 → **Hive, Pig, Cascading, Crunch** 등 고수준 언어/API 확산.
- **데이터플로 전환**: **Tez**가 기존 고수준 작업을 코드 거의 수정 없이 데이터플로로 실행하게 함; **Spark/Flink**도 자체 고수준 API 제공.
- **표현 방식과 이점**: 조인·그룹화·필터 등 **관계형 블록**으로 파이프라인을 선언적으로 구성, 대화형 사용 지원 → **개발 생산성과 운영 효율** 상승.

### 4-10. 선언형 질의 언어로 전환

- **조인 최적화 자동화**: 관계형 연산자(조인 등)로 표현하면 프레임워크가 입력 특성을 보고 적절한 조인 알고리즘과 순서를 **비용 기반으로 자동 결정**한다. 하이브·스파크·플링크가 이를 지원.
- **완전 SQL은 아님**: 데이터플로우/맵리듀스 계열은 SQL처럼 전부 선언형이 아니라, **사용자 정의 함수(UDF)** 를 통해 레코드(또는 튜플) 집합을 입력·출력하는 **함수 골격**을 호출하는 방식이 핵심이다.
- **선언형 보강의 이점**: 전부 코드로만 쓰면 필터/프로젝션처럼 간단한 작업도 **CPU 오버헤드**가 크다. 이를 **선언형으로 표현**하면 저장계층 필터 푸시다운 등 **질의 최적화**가 가능하다.

### 4-11. 다양한 분야를 지원하기 위한 전문화

- 임의 코드를 실행할 수 있는 **확장성**은 유용하지만, 반복적으로 등장하는 **공통 처리 패턴**이 많으므로 재사용 가능한 **공통 빌딩 블록**을 마련할 가치가 있다.
- 전통적 **MPP 데이터베이스**는 BI 분석/리포팅 수요에 주로 맞춰 왔지만, 일괄 처리의 적용 범위는 그보다 훨씬 넓다.
- 중요성이 커진 다른 영역으로 **통계·수치 알고리즘**, **추천/머신러닝**이 있다.
    - 예: **Mahout**(맵리듀스·스파크·플링크 상에서 동작하는 다양한 ML 알고리즘 제공),
        
        **MADlib**(관계형 MPP DB 내부 확장으로 유사 기능 제공, 예: HAWQ 내 구현).
        
- 대표적 알고리즘 **k-최근접 이웃(k-NN)** 은 근접한 아이템을 찾는 일반적 검색/유사도 분석에 쓰이며, 동일한 거리 계산을 반복하는 **비교 기반 작업**이 핵심이다.
- 일괄 처리 엔진은 각 영역에서 필요한 알고리즘을 **분산 수행**하는 데 사용되며, 고수준 **선언적 연산자**(또는 한층 프로그래밍이 가능한 인터페이스)를 갖춘 **MPP DB**와 점점 유사해지고 있다.
- 공통점: 두 계열 모두 **거대한 데이터셋을 저장·처리**하는 시스템이며, 분야별 라이브러리/블록을 통해 전문화를 확장한다.

## 5. 정리

- **유닉스 철학 → 분산 일괄 처리로**: awk/grep/sort 같은 작은 도구들을 파일/파이프로 연결하듯, 분산 일괄 처리에서도 **입력은 불변 데이터, 출력은 다음 작업의 입력**으로 이어 붙인다.
- **인터페이스**: 맵리듀스의 인터페이스는 **분산 파일 시스템(HDFS)**. 데이터플로 엔진은 자체 전송 메커니즘으로 **중간 상태 구체화(쓰기)** 를 줄이지만, **초기 입력과 최종 출력**은 여전히 HDFS에 기록한다.
- **핵심 과제 1. 파티셔닝:** 맵 출력은 **키 기반 파티셔닝**(리듀서 개수 설정 가능)으로 같은 키가 같은 파티션에 모인다.
- **핵심 과제 2. 내결함성:** 맵리듀스- 중간 결과를 **디스크에 기록**해 태스크 실패 시 전체 작업을 재시작하지 않고 **부분 재실행 /** 데이터플로 엔진- 가능하면 **중간 상태를 메모리로 유지**해 재계산 양을 줄인다(결정적 연산이면 재실행 쉬움).
- **대표 조인 알고리즘**
    - **정렬 병합 조인**: 맵에서 키 추출·파티셔닝·정렬 후 리듀서에서 병합.
    - **브로드캐스트 해시 조인**: 작은 입력을 모든 파티션에 **복제**해 해시 테이블로 결합.
    - **파티션 해시 조인**: 양쪽 입력을 **같은 키 기준**으로 해시 파티셔닝하여 각 파티션에서 독립적으로 조인.
- **프레임워크 가정**: 콜백(맵/리듀스 등) 외 **부수효과 없음**, 실패 시 **안전하게 재시도**. 여러 태스크가 성공해도 **실제 출력은 하나**만 남도록 보장.
- **일괄 처리의 본질**: **입력이 고정된 크기**(로그 스냅숏/DB 스냅숏 등)라 작업이 **끝**난다. 출력은 **입력으로부터 파생**된다.
- **다음 장(스트림 처리)과의 차이**: 스트림은 **입력이 무한**하므로 작업이 끝나지 않으며, 설계 방식이 크게 달라진다.

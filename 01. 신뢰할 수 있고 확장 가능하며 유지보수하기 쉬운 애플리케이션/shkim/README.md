# 신뢰할 수 있고 확장 가능하며 유지보수하기 쉬운 애플리케이션

오늘날 많은 애플리케이션은 **계산 중심**과는 다르게 **데이터 중심적**이다. <br>
이러한 애플리케이션의 경우 CPU 성능은 애플리케이션을 제한하는 요소가 아니며, 더 큰 문제는 보통 데이터의 양, 데이터의 복잡도, 데이터의 변화 속도다. <br>
일반적으로 데이터 중심 애플리케이션은 공통으로 필요로 하는 기능을 제공하는 표준 구성 요소로 만든다. (예를 들면 아래와 같다.)

- 데이터베이스
- 캐시
- 검색 색인
- 스트림 처리
- 배치 처리


> 애플리케이션마다 요구사항이 다르기 때문에 데이터베이스 시스템 또한 저마다 다양한 특성을 가지고 있다. <br>
> 캐싱을 위한 다양한 접근 방식과 검색 색인을 구축하는 여러 가지 방법 등이 있다. <br>
> 애플리케이션을 만들 때 어떤 도구와 어떤 접근 방식이 수행 중인 작업에 가장 적합한지 생각해야 한다.

## 데이터 시스템에 대한 생각

데이터베이스와 메시지 큐는 표면적으로 비슷하더라도 매우 다른 접근 패턴을 갖고 있어 서로 다른 성능 특성이 있기 때문에 구현 방식이 매우 다르다. <br>
그러면 모든 것을 왜 **데이터 시스템**이라는 포괄적 용어로 묶어야 할까? <br>
새로운 도구들은 다양한 사용 사례에 최적화됐기 때문에 더이상 전통적인 분류에 들어맞지 않는다. <br>
예를 들어 메시지 큐로 사용하는 데이터스토어인 레디스가 있고, 데이터베이스처럼 지속성을 보장하는 메시지 큐인 카프카가 있다.

두 번째로 점점 더 많은 애플리케이션이 단일 도구로는 더 이상 데이터 처리와 저장 모두를 만족시킬 수 없는 과도하고 광범위한 요구사항을 갖고 있다. <br>
**대신 작업은 단일 도구에서 효율적으로 수행할 수 있는 태스크로 나누고 다양한 도구들은 애플리케이션 코드를 이용해 서로 연결한다.**

<img width="692" height="460" alt="Image" src="https://github.com/user-attachments/assets/d620d1a8-f2eb-423c-9030-5cf33229c2d6" />

이 책에서는 대부분의 소프트웨어 시스템에서 중요하게 여기는 세 가지 관심사에 중점을 둔다.

- **신뢰성**: 하드웨어나 소프트웨어 결함, 휴먼 에러같은 역경에 직면하더라도 시스템은 자속적으로 올바르게 동작해야 한다.
- **확장성**: 시스템의 데이터 양, 트래픽 양, 복잡도가 증가하면서 이를 처리할 수 있는 적절한 방법이 있어야 한다.
- **유지보수성**: 시간이 지남에 따라 여러 다양한 사람들이 시스템 상에서 작업할 것이기 때문에, 모든 사용자가 시스템상에서 생산적으로 작업할 수 있게 해야한다.

## 신뢰성

소프트웨어의 경우 일반적인 신뢰성에 대한 기대치는 다음과 같다.

- 애플리케이션은 사용자가 기대한 기능을 수행한다.
- 시스템은 사용자가 범한 실수나 예상치 못한 소프트웨어 사용법을 허용할 수 있다.
- 시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다.
- 시스템은 허가되지 않은 접근과 오남용을 방지한다.

**대략 무언가 잘못되더라도 지속적으로 올바르게 동작함을 신뢰성의 의미로 이해할 수 있다.** <br>
잘못될 수 있는 일을 **결함**이라고 부르고, 결함을 예측하고 대처할 수 있는 시스템을 **내결함성 또는 탄력성**을 지녔다고 말한다.

### 하드웨어 결함

하드디스크가 고장 나고, 램에 결함이 생기고, 대규모 정전 사태가 발생하고, 누군가가 네트워크 케이블을 잘못 뽑는 것과 같은 결함을 말한다. <br>
시스템 장애율을 줄이기 위한 첫 번째 대응으로 각 하드웨어 구성 요소에 **중복**을 추가하는 방법이 일반적이다. <br>
구성 요소 하나가 죽으면 고장 난 구성 요소가 교체되는 동안 중복된 구성 요소를 대신 사용할 수 있다.

### 소프트웨어 오류

하드웨어 결함은 무작위적이고 독립적이라고 생각된다. 한 장비의 디스크에 장애가 있더라도 다른 장비의 디스크에 장애가 발생하지는 않는다. <br>
또 다른 부류의 결함으로 시스템 내 체계적 오류가 있다. 이 결함은 예상하기가 더 어렵고 노드 간 상관관계 때문에 상관관계 없는 하드웨어 결함보다 오히려 시스템 오류를 더욱 많이 유발하는 경향이 있다. <br>
예를 들면 다음과 같다.

- 잘못된 특정 입력이 있을 때 모든 애플리케이션 서버 인스턴스가 죽는 소프트웨어 버그.
- CPU 시간, 메모리, 디스크 공간, 네트워크 대역폭처럼 공유 자원을 과도하게 사용하는 일부 프로세스.
- 시스템의 속도가 느려져 반응이 없거나 잘못된 응답을 반환하는 서비스.
- 한 구성 요소의 작은 결함이 다른 구성 요소의 결함을 야기하고 차례차례 더 많은 결함이 발생하는 연쇄 장애.

> 소프트웨어의 체계적 오류 문제는 신속한 해결책이 없다. <br>
> 시스템의 가정과 상호작용에 대해 주의 깊게 생각하기, 빈틈없는 테스트, 프로세스 격리, 죽은 프로세스의 재시작 허용, <br>
> 프로덕션 환경에서 시스템 동작의 측정, 모니터링, 분석하기와 같은 여러 작은 일들이 문제 해결에 도움을 줄 수 있다.

### 인적 오류 (human error)

사람은 소프트웨어 시스템을 설계하고 구축하며, 운영자로서 시스템을 계속 운영한다. 이들이 최선의 의도를 갖고 있어도 사람은 미덥지 않다고 알려져 있다. <br>
사람이 미덥지 않음에도 시스템을 어떻게 신뢰성 있게 만들까? 최고의 시스템은 다양한 접근 방식을 결합한다.

- 오류의 가능성을 최소화하는 방향으로 시스템을 설계하라. 예를 들어 잘 설계된 추상화, api, 관리 인터페이스를 사용하라.
- 사람이 가장 많이 실수하는 장소에서 사람의 실수로 장애가 발생할 수 있는 부분을 분리하라. 특히 실제 데이터를 사용해 안전하게 살펴보고 실험할 수 있지만 실제 사용자에게는 영향이 없는 비 프로덕션 샌드박스를 제공하라.
- 단위 테스트부터 전체 시스템 통합 테스트와 수동 테스트까지 모든 수준에서 철저하게 테스트하라.
- 장애 발생의 영향을 최소화하기 위해 인적 오류를 빠르고 쉽게 복구할 수 있게 하라. 
- 성능 지표와 오류율 같은 상세하고 명확한 모니터링 대책을 마련하라.

## 확장성

성능 저하를 유발하는 흔한 이유 중 하나는 부하 증가다. <br>
**확장성**은 증가한 부하에 대처하는 시스템 능력을 설명하는 데 사용하는 용어이다.

### 부하 기술하기

부하는 **부하 매개변수**라 부르는 몇 개의 숫자로 나타낼 수 있다. <br>
**가장 적합한 부하 매개변수 선택은 시스템 설계에 따라 달라진다.** <br>
부하 매개변수로 웹 서버의 초당 요청 수, 데이터베이스의 읽기 대 쓰기 비율, 대화방의 동시 활성 사용자, 캐시 적중률 등이 될 수 있다.

이를 조금 더 구체적으로 설명하기 위해 트위터를 예로 살펴보자. <br>
**트위터의 확장성 문제는 트윗 양이 아닌 팬아웃 때문이다.** <br>
개별 사용자는 많은 사람을 팔로우하고 많은 사람이 개별 사용자를 팔로우한다. 이 두 가지 동작을 구현하는 방법은 크게 두 가지다.

1. 트윗 작성은 간단히 새로운 트윗을 트윗 전역 컬렉션에 삽입한다. 사용자가 자신의 홈 타임라인을 요청하면 팔로우하는 모든 사람을 찾고, 이 사람들의 모든 트윗을 찾아 시간순으로 정렬해서 합친다.
2. 각 수신 사용자용 트윗 우편함처럼 개별 사용자의 홈 타임라인 캐시를 유지한다. 사용자가 트윗을 작성하면 해당 사용자를 팔로우하는 사람을 모두 찾고 팔로워 각자의 홈 타임라인 캐시에 새로운 트윗을 삽입한다.

트위터의 첫 번째 버전은 접근 방식 1을 사용했는데, 시스템이 홈 타임라인 질의 부하를 버텨내기 위해 고군분투해야 했고, 그 결과 접근 방식 2로 전환했다. <br>
평균적으로 트윗 게시 요청량이 홈 타임 라인 읽기 요청량에 비해 수백 배 적기 때문에 접근 방식 2가 훨씬 잘 동작한다. <br>
그래서 이 경우에는 쓰기 시점에 더 많은 일을 하고, 읽기 시점에 적은 일을 하는 것이 바람직하다.

**하지만 접근 방식 2의 불리한 점은 이제 트윗 작성이 많은 부가 작업을 필요로 한다는 점이다.** <br>
**트위터 사례에서 사용자당 팔로워의 분포는 팬아웃 부하를 결정하기 때문에, 확장성을 논의할때 핵심 부하 매개변수가 된다.**

접근 방식 2가 견고하게 구현돼 트위터는 두 접근 방식의 혼합형으로 바꾸고 있다. <br>
대부분 사용자의 트윗은 계속해서 사람들이 작성할 때 홈 타임라인에 펼쳐지지만 팔로워 수가 매우 많은 소수 사용자는 팬아웃에서 제외된다. <br>
사용자가 팔로우한 유명인의 트윗은 별도로 가져와 접근 방식 1처럼 읽는 시점에 사용자의 홈 타임라인에 합친다.

### 성능 기술하기

<img width="646" height="231" alt="Image" src="https://github.com/user-attachments/assets/a58c399a-8061-45c4-87a4-e1bd9fe23813" />

일단 시스템 부하를 기술하면 부하가 증가할 때 어떤 일이 일어나는지 조사할 수 있다.

- 부하 매개변수를 증가시키고 시스템 자원은 변경하지 않고 유지하면 시스템 성능은 어떻게 영향을 받을까 ?
- 부하 매개변수를 증가시켰을때 성능이 변하지 않고 유지되길 원한다면 자원을 얼마나 많이 늘려야 할까 ?

하둡같은 일괄 처리 시스템은 보통 **처리량**에 관심을 가진다. <br>
온라인 시스템에서 더 중요한 사항은 서비스 **응답 시간**이다. <br>
클라이언트가 몇 번이고 반복해서 동일한 요청을 하더라도 매번 응답 시간이 다르다. 실제로 다양한 요청을 다루는 시스템에서 응답 시간은 많이 변한다. <br>
그러므로 응답 시간은 단일 숫자가 아니라 측정 가능한 값의 분포로 생각해야 한다. <br>
**보고된 서비스 평균 응답 시간을 살피는 일은 일반적이다.**

사용자가 보통 얼마나 오랫동안 기다려야 하는지 알고 싶다면 중앙값이 좋은 지표다. 사용자 요청의 절반은 중앙값 응답 시간 미만= 제공되고, 나머지 반은 중앙값보다 오래 걸린다. <br>
중앙값은 50분위로서 p50으로 축약할 수 있다. 중앙값은 단일 요청을 참고한다는 점을 주의하자.

특이 값이 얼마나 좋지 않은지 알아보려면 상위 백분위를 살펴보는 것도 좋다. <br>
이때 사용하는 백분위는 95분위, 99분위. 99.9분위가 일반적이다(축약해서 p95, p99, p999)

**꼬리 지연 시간(tail latency)** 으로 알려진 상위 백분위 응답 시간은 서비스의 사용자 경험에 직접 영향을 주기 때문에 중요하다. <br>
예를 들어 아마존은 내부 서비스의 응답 시간 요구사항을 99.9분위로 기술한다. 99.9분위는 요청 1,000개 중 1개만 영향이 있음에도 말이다.

### 부하 대응 접근 방식

사람들은 확장성과 관련해 용량 확장(scale up), 규모 확장(scale out)으로 구분해서 말하곤 한다. 다수의 장비에 부하를 분산하는 아키텍처를 비공유 아키텍처라 부른다. <br>
단일 장비에서 수행될 수 있는 시스템은 보통 간단하지만 고사양 장비는 매우 비싸기 때문에 상당히 집약된 작업 부하는 대개 규모 확장을 피하지 못한다.

**대용량 데이터와 트래픽을 다루지 않는 사용 사례에도 분산 데이터 시스템이 향후 기본 아키텍처로 자리 잡을 가능성이 있다.** <br>
대개 대규모로 동작하는 시스템의 아키텍처는 해당 시스템을 사용하는 애플리케이션에 특화돼 있다. <br>
아키텍처를 결정하는 요소는 읽기의 양, 쓰기의 양, 저장할 데이터의 양, 데이터의 복잡도, 응답시간 요구사항, 접근 패턴 등이 있다.

> 특정 애플리케이션에 적합한 확장성을 갖춘 아키텍처는 주요 동작이 무엇이고 잘 하지 않는 동작이 무엇인지에 대한 가정을 바탕으로 구축한다. 이 가정은 곧 부하 매개변수가 된다. <br>
> 이 가정이 잘못되면 확장에 대한 엔지니어링 노력은 헛수고가 되고 최악의 경우 역효과를 낳는다. <br>
> 스타트업 초기 단계나 검증되지 않은 제품의 경우에 미래를 가정한 부하에 대비해 확장하기보다는 빠르게 반복해서 제품 기능을 개선하는 작업이 좀 더 중요하다.

## 유지보수성

유지보수에는 버그 수정, 시스템 운영 유지, 장애 조사, 새로운 플랫폼 적응, 새 사용 사례를 위한 변경, 기술 채무 상환, 새로운 기능 추가 등이 있다. <br>
유지보수 중 고통을 최소화하고 레거시 소프트웨어를 직접 만들지 않게끔 소프트웨어를 설계할 수 있다. 그러기 위해 주의를 기울여야할 소프트웨어 시스템 설계 원칙은 다음 세가지다.

- **운용성**: 운영팀이 시스템을 원활하게 운영할 수 있게 쉽게 만들어라.
- **단순성**: 시스템에서 복잡도를 최대한 제거해 새로운 엔지니어가 시스템을 이해하기 쉽게 만들어라.
- **발전성**: 엔지니어가 이후에 사스템을 쉽게 변경할 수 있게 하라. 그래야 요구사항 변경 같은 예기치 않은 사용 사례를 적용하기가 쉽다.

### 운용성: 운영의 편리함 만들기

**좋은 운영은 종종 불완전한 소프트웨어의 제약을 피하는 대안이 될 수 있다. 하지만 좋은 소프트웨어라도 나쁘게 운영할 경우 작동을 신뢰할 수 없다.** <br>
시스템이 지속해서 원활하게 작동하려면 운영팀이 필수다. 좋은 운영팀은 일반적으로 다음과 같은 작업 등을 책임진다.

- 시스템 상태를 모니터링하고 상태가 좋지 않다면 빠르게 서비스를 복원
- 시스템 장애. 성능 저하 등의 문제의 원인을 추적
- 보안 패치를 포함해 소프트웨어와 플랫폼을 최신 상태로 유지
- 다른 시스템이 서로 어떻게 영향을 주는지 확인해 문제가 생길 수 있는 변경 사항을 손상을 입히기 전에 차단
- 미래에 발생 가능한 문제를 예측해 문제가 발생하기 전에 해결(예를 들어 용량 계획)
- 배포, 설정 관리 등을 위한 모범 사례와 도구를 마련
- 애플리케이션을 특정 플랫폼에서 다른 플랫폼으로 이동하는 등 복잡한 유지보수 태스크를 수행
- 설정 변경으로 생기는 시스템 보안 유자보수
- 예측 가능한 운영과 안정적인 서비스 환경을 유지하기 위한 절차 정의
- 개인 인사 이동에도 시스템에 대한 조직의 지식을 보존함

### 단순성: 복잡도 관리

복잡도는 다양한 증상으로 나타난다. 상태 공간의 급증, 모듈간 강한 결합, 복잡한 의존성, 일관성 없는 명명과 용어, 성능 문제 해결을 목표로 한 해킹, 임시방편으로 문제를 해결한 특수 사례 등이 이런 증상이다. <br>
복잡도 때문에 시스템 유지보수가 어려울 때 예산과 일정이 초과되곤 한다. 복잡한 소프트웨어에서는 변경이 있을 때 버그가 생길 위험이 더 크다. 

우발적 복잡도를 제거하기 위한 최상의 도구는 **추상화**다. <br>
좋은 추상화는 깔끔하고 직관적인 외관 아래로 많은 세부 구현을 숨길 수 있다. 또한 좋은 추상화는 다른 다양한 애플리케이션에서도 사용 가능하다.

### 발전성: 변화를 쉽게 만들기

시스템의 요구사항이 영원히 바뀌지 않을 가능성은 매우 적다. 시스템의 요구사항이 끊임없이 변할 가능성이 훨씬 크다. <br>
조직 프로세스 측면에서 **애자일** 작업 패턴은 변화에 적응하기 위한 프레임워크를 제공한다. <br>
이런 애자일 기법에 대한 설명은 대부분 매우 작고, 로컬 규모에 초점을 맞추고 있다. <br>
이 책에서는 다양한 애플리케이션이나 다른 특성을 가진 서비스로 구성된 대규모 데이터 시스템 수준에서 민첩성을 높이는 방법을 찾는다.









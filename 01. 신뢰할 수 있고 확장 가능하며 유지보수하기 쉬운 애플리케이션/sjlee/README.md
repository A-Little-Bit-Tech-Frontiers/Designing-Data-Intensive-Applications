오늘날 많은 애플리케이션은 계산 중심이 아니라 데이터 중심적이다. 이러한 애플리케이션은 CPU 성능보다 데이터의 양, 데이터 복잡도, 데이터 변화가 병목지점인 셈이다.

일반적으로 데이터 중심 애플리케이션은 공통 기능을 제공하는 **표준 구성 요소**로 만든다. 예를 들어, 많은 애플리케이션은 아래와 같은 것들을 필요로 한다.

- 애플리케이션에서 나중에 다시 데이터를 찾을 수 있게 데이터를 저장 → 데이터베이스
- 읽기 성능 최적화를 위해 값비싼 연산 결과를 저장 → 캐시
- 키워드로 데이터 검색 혹은 다양한 방법의 필터링을 제공 → 검색 색인
- 비동기 처리를 위해 다른 프로세스로 메시지 보내기 → 스트림 처리

**데이터 시스템이 성공적으로 추상화**된 덕분에 위와 같이 요구사항에 대한 답을 명확하게 낼 수 있는 것임.

데이터 시스템의 원칙(principle)과 실용성(practicality), 그리고 이를 활용한 데이터 중심 애플리케이션을 개발하는 방법을 다루는게 이 책의 목적이다. 다양한 도구(캐시, 스트림 처리, 데이터베이스 등)가 지닌 공통적인 것들은 무엇이고 서로 구별되는 것은 무엇인지 등을 알아본다.

# 1. 데이터 시스템에 대한 생각

데이터베이스와 메시지 큐는 표면적으로 “데이터를 저장”한다는 의미에서 비슷하지만, 매우 다른 접근 패턴을 갖고 있다. 

그러면 모든 것을 왜 “데이터 시스템”이라는 용어로 묶어야 하나?

새로운 도구들은 다양한 use-case에 최적화 되어 있다. 메시지 큐로 사용되는 데이터스토어인 레디스도 있고, DB처럼 지속성(durability)을 보장하는 메시지 큐인 카프카 등도 있다.
또 다르게 효율적으로 태스크를 수행하기 위해 도구를 분리한다면, 캐시 계층을 둘 수도 있고 검색을 위한 검색 서버를 둘 수도 있다.
즉, 아래 그림과 같이 다양한 구성 요소를 결합해 하나의 데이터 시스템 아키텍처를 구성할 수 있다.

<img width="395" height="258" alt="image" src="https://github.com/user-attachments/assets/3c40ebbd-113b-4b6c-beec-456a95ee3e91" />


→ 개발자는 애플리케이션 개발자뿐만 아니라 데이터 시스템 설계자이기도 함.

데이터 시스템을 설계하면서 아래와 같은 질문에 답 할 수 있어야 한다.

- 내부적으로 문제가 있어도 데이터를 정확하고 안전하게 유지하려면 어떻게 해야 할까?
- 시스템의 일부 성능이 저하되더라도 클라이언트에 일관되게 좋은 성능을 어떻게 제공할 수 있을까?
- 부하 증가를 다루기 위해 어떻게 규모를 확장할 수 있을까?
- 서비스를 위한 좋은 API는 어떤 모습일까?

또한, 소프트웨어 시스템에서 중요하게 여기는 세 가지 관심사는 아래와 같다.

- **신뢰성(Reliability)**
    
    하드웨어나 소프트웨어 결함 등 어떤 결함이 있더라도 지속적으로 올바르게 동작해야 한다.
    
- **확장성(Scalability)**
    
    시스템의 데이터 양, 트래픽 양, 복잡도가 증가하면서 이를 처리할 수 있는 적절한 방법이 있어야 한다.
    
- **유지보수성(Maintainability)**
    
    다양한 사람들이 시스템 작업을 생산적으로 할 수 있어야 한다.
    

# 2. 신뢰성

소프트웨어가 기대하는 신뢰는 아래와 같다.

- 애플리케이션은 사용자가 기대한 기능을 수행한다.
- 시스템은 사용자가 범한 실수나 예상못한 소프트웨어 사용법을 허용할 수 있다. → 엣지 케이스 고려되어야 한다는 의미인듯
- 시스템 성능은 예상된 부하와 데이터 양에서 필수적인 사용 사례를 충분히 만족한다.
- 시스템은 허가되지 않은 접근과 오남용을 방지한다.

잘못될 수 있는 일을 **결함(fault)**라고 부른다. 이 결함을 예측하고 대처할 수 있는 시스템을 내결함성 또는 탄력성을 지녔다고 할 수 있다. 이 결함(fault)은 **장애(failure)**와는 다르다. 결함은 사양에서 벗어난 시스템의 한 구성요소인 반면에, 장애는 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우다. 대개 결함으로 인해 장애가 발생하지 않게끔 주의해야 한다.

## 2.1 하드웨어 결함

하드디스크의 평균 장애 시간(mean time to failure, MTTF ⇒ 가동시간/장애횟수)은 약 10 ~ 50년으로 보고됐다. 따라서 10,000개의 디스크로 구성된 저장 클러스터는 평균적으로 하루 한 개의 디스크가 죽는 셈이다. (10,000 / (30*365) = 0.9XX)

시스템 장애율을 줄이기 위한 대응으로 각 하드웨어 구성 요소에 중복(redundancy)을 추가하는 방법이 일반적이다. 구성 요소가 죽으면 고장 난 구성 요소가 교체되는 동안 중복된 녀석을 사용하는 전략이다.
최근까지 단일 장비의 전체 장애는 매우 드물었기 때문에 위 방식으로 충분했다. 새 장비에 백업을 매우 빠르게 복원할 수 있는 한, 다운타임은 치명적이지 않았다.

하지만, 데이터 양이 많아지고 애플리케이션 계산 요구가 늘어나면서 많은 수의 장비를 사용하게 되었다. 이에 따라 하드웨어 결함율도 증가했다. AWS와 같은 클라우드 플랫폼은 단일 장비 신뢰성보다 유연성과 탄력성을 우선적으로 설계되었다. 

## 2.2 소프트웨어 오류

하드웨어 결함은 다른 결함에 비해 비교적 무작위적이고 독립적이다. 즉, 한 장비의 디스크가 장애났다고 해서 다른 장비의 디스크가 장애 나진 않는다.

하지만, **시스템 내 체계적 오류(systematic error)**로 분류되는 결함은 이야기가 다르다. 이 결함은 예측이 어렵고, 노드 간 상관관계 때문에 시스템 오류를 더 많이 유발하는 경향이 있다. 예를 들어 아래와 같음

- 잘못된 특정 입력에 의해 모든 애플리케이션 서버 인스턴스가 죽는 버그
- CPU 시간, 메모리, 디스크 공간, 네트워크 대역폭처럼 공유 자원을 과도하게 사용하는 일부 프로세스
- 시스템의 속도가 느려져 반응이 없거나 잘못된 응답을 반환하는 서비스
- 연쇄 장애

## 2.3 인적 오류

- 시스템은 사람이 설계·운영하며, 사람의 실수로 인해 오류가 발생할 수 있음.
- 연구에 따르면 운영자의 설정 오류는 전체 중단 원인의 10~25% 차지.
- 따라서 “사람이 실수하더라도 시스템을 어떻게 신뢰성 있게 만들 것인가?”가 핵심 과제임.

# 3. 확장성

시스템의 성능 저하를 유발하는 이유 중 하나는 부하 증가다. 확장성은 이러한 문제에 대처하는 능력을 말한다. “시스템이 특정 방식으로 커지면 이에 대처하기 위한 선택은 무엇일까?” 에 대한 답을 고려해야 한다.

## 3.1 부하 기술하기

부하는 **부하 매개변수(load parameter)**라 부르는 몇 개의 숫자로 나타낼 수 있다. 이 매개변수로는 웹 서버의 초당 요청 수, 데이터베이스 읽기 대 쓰기 비율, 동시 활성 사용자 수, 캐시 적중률 등이 있다.
가장 적합한 부하 매개변수 선택은 시스템 설계에 따라 달라진다. 평균적인 경우가 중요할 수도 있고 소수의 극단적인 경우가 중요할 수도 있다.

트위터를 예시로 하여금 구체적으로 살펴보자.

- 트윗 작성
    
    사용자는 팔로워에게 새로운 메시지를 게시할 수 있다. (평균 초당 4.6k 요청, 피크일 때 초당 12k 요청 이상)
    
- 홈 타임라인
    
    사용자는 팔로우한 사람이 작성한 트윗을 볼 수 있다. (초당 300k 요청)
    

단순히 초당 12k 만큼의 쓰기 처리는 간단하다. 하지만, 트위터의 확장성 문제는 주로 트윗 양이 아닌 팬-아웃 때문이다.
개별 사용자는 많은 사람을 팔로우하고 많은 사람이 개별 사용자를 팔로우한다. 크게 두 가지 방식으로 구현이 가능하다.

1. 트윗 작성은 트윗 전역 컬렉션에 새로 삽입한다. **사용자가 자신의 홈 타임라인을 요청하면, 팔로우 하는 모든 사람을 찾고 그 사람들의 모든 트윗을 찾아 시간순으로 정렬**한다. 
    
    <img width="580" height="188" alt="image" src="https://github.com/user-attachments/assets/d7345288-3f8c-4133-8925-c24eb12040d9" />

    
2. 개별 사용자의 홈 타임라인 캐시를 유지한다. **사용자가 트윗을 작성하면 해당 사용자를 팔로우하는 사람을 모두 찾고, 팔로워 각자의 홈 타임라인 캐시에 트윗을 삽입**한다. → 홈 타임라인의 읽기 요청은 결과를 미리 계산해놨기 때문에 비용이 저렴하다.
    
    <img width="607" height="205" alt="image" src="https://github.com/user-attachments/assets/26006d34-9017-41d0-a7a3-453e0a76f7db" />

    

2번 방식으로 하면 조회 성능은 올라가지만 쓰기 작업에 대해 많은 비용이 든다. 평균적으로 75명의 팔로워가 있다고 가정하면, 초당 4.6k 트윗은 홈 타임라인 캐시에 초당 345k 건의 쓰기가 된다. 

만약, 팔로워가 3천만이 넘는 일부 사용자의 경우 어떨까? ⇒ 단일 트윗이 3천만건 이상의 쓰기 작업으로 이어질 수 있는 것이다.

트위터 사례에서 **사용자당 팔로워 분포**는 팬-아웃 부하를 결정하기 때문에 확장성을 논의할 때 **핵심 부하 매개변수**가 된다. 

## 3.2 성능 기술하기

시스템 부하를 기술하면 부하가 증가할 때 어떤 일이 일어나는지 조사할 수 있다. 

- 부하 매개변수를 증가시키고 시스템 자원은 변경하지 않은 상태라면, 시스템 성능은 어떻게 영향을 받을까?
- 부하 매개변수를 증가시켰을 때 성능이 변하지 않고 유지되길 원한다면 자원을 얼마나 많이 늘려야 할까?

두 질문 모두 **성능 수치**가 필요하다.

일괄 처리 시스템은 보통 **처리량(throughput)**에 관심을 가지며, 온라인 시스템에서 더 중요한 사항은 서비스 **응답 시간(response time)**이다. 
클라이언트가 매번 같은 요청을 해도 매번 응답시간은 다르다. 그러므로 **응답 시간은 단일 숫자가 아니라 측정 가능한 값의 분포로 생각**해야 한다. 

<img width="603" height="181" alt="image" src="https://github.com/user-attachments/assets/e1bb656c-fd63-4976-82d6-6389d3b2ee49" />


위와 같이 같은 요청에도 서로 다른 응답시간을 보이고 있다. 가끔 꽤 오래 걸리는 특이 값(outlier)도 존재한다. 백그라운드 프로세스의 컨텍스트 스위치, 네트워크 패킷 손실과 TCP 재전송, 가비지 컬렉션 휴지, 디스크 page fault 등 다양한 원인이 있을 수 있다. 

**평균** 응답 시간을 살피는 것은 일반적이지만, 좋은 지표는 아닐 수 있다. 일반적인 응답 시간을 알고 싶다면, **평균 보다는 백분위를 사용하는 편이 더 좋다.** 평균은 “얼마나 많은” 사용자가 실제로 지연을 경험했는지 알 수 없기 때문이다.

백분위를 기준으로 하여, 여러 의미 있는 값들이 있다.

- 중앙값(p50, 50분위): 가장 빠른 시간부터 가장 늦은 시간까지의 정렬했을 때의 중간 지점
- p95, p99, p999: 요청의 95%, 99%, 99.9%가 특정 기준치보다 더 빠르면 그 기준치가 각 백분위의 응답 시간 기준치가 된다. 
예를 들어, p95 응답시간이 1.5s ⇒ 100 개의 요청 중 95개는 1.5초 미만, 5개는 1.5초 이상이라는 의미
    
    이와 같이 이러한 상위 백분위는 특이 값이 얼마나 좋지 않은지 알아보는 좋은 지표가 되곤 한다.
    

꼬리 지연 시간(tail latency)으로 알려진 상위 백분위 응답 시간은 사용자 경험에 직접적인 영향을 주는 지표다.
아마존을 예시로 들어보자.
보통 응답 시간이 느린 요청을 경험한 고객들은 데이터를 많이 갖고 있는 주요 고객일 가능성이 높다. 
그리고 이 고객들의 경험을 만족시키기 위해, 아마존은 p999(1,000개의 요청 중 1개)를 기준으로 요구사항을 기술한다고 한다.

큐 대기 지연은 높은 백분위에서 응답 시간의 상당 부분을 차지한다. 서버는 병렬로 적은 수의 작업만 처리할 수 있기 때문에 소수의 느린 요청처리에 의해 후속 처리가 지체된다. ⇒ 이를 **선두 차단(head-of-line blocking)**이라고 함.

<img width="643" height="287" alt="image" src="https://github.com/user-attachments/assets/21277867-c1d3-4f3f-8361-cad290f01633" />


요청을 처리하기 위해 여러 번 백엔드를 호출이 필요한 상황에서, 느린 요청이 단 하나라도 포함되어 있다면 최종 사용자는 느린 응답시간을 갖게 된다.

## 3.3 부하 대응 접근 방식

### 3.3.1 부하 대응 방식

- 시스템이 점점 더 많은 부하를 처리하기 위해서는 **확장성(Scalability)** 이 중요함.
- **확장의 두 가지 접근 방식**
    - **용량 확장 (Scaling up, 수직 확장)**: 더 강력한 장비로 교체.
    - **규모 확장 (Scaling out, 수평 확장)**: 여러 대의 장비에 부하를 분산.
- 수평 확장은 다수의 장비에 작업을 분산시키는 **비공유(shared-nothing) 아키텍처**와 밀접

### 3.3.2 탄력성(Elasticity)

- 일부 시스템은 **자동으로 리소스를 증감**(자동 확장)할 수 있어 유연함.
- 하지만 많은 시스템은 여전히 수동으로 확장을 결정해야 함.
- 부하를 예측하기 어렵거나 급격한 증가가 예상될 경우, 탄력적인 자동 확장이 더 적합

### 3.3.3 상태 관리

- **Stateless 서비스**: 확장이 비교적 쉽다.
- **Stateful 데이터 시스템(예: DB)**: 확장이 매우 복잡하고 비용이 크다.
    - 따라서 데이터베이스는 여전히 단일 노드 기반 확장(수직 확장)이 흔히 사용됨.

### 3.3.4 확장성의 제약

- 모든 상황에 맞는 만능 확장 아키텍처는 존재하지 않음 (**one-size-fits-all 없음**).
- 데이터 저장량, 복제도, 응답 시간, 접근 패턴 등 다양한 요소가 고려되어야 함.
- 예: 초당 10만 건의 작은 요청 처리 시스템과, 소수의 대용량 요청 처리 시스템은 전혀 다른 아키텍처를 요구.

# 4. 유지보수성

운용성

- 운영팀이 시스템을 원할하게 운영할 수 있도록 쉽게 만들어라.

단순성

- 시스템에서 복잡도를 최대한 제거해 새로운 엔지니어가 시스템을 이해하기 쉽게 만들어라.

발전성

- 시스템을 쉽게 변경할 수 있도록 만들어라.

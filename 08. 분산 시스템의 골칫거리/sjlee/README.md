# 1. 결함과 부분장애

하드웨어는 결정적인 성격을 가진다. 즉, 하드웨어가 올바르게 동작하면 같은 연산은 항상 같은 결과를 낸다. 만약 하드웨어 문제가 있다면 완전히 실패한다.

→ 컴퓨터 설계 의도임. 잘못된 결과보다 완전히 동작하지 않기를 바람.

문제는 분산 시스템이다.

분산 시스템에서는 시스템의 어떤 부분은 잘 동작하지만 다른 부분은 예측할 수 없는 방식으로 고장나는 것도 무리가 아니다. 이를 **부분 장애**라고 한다. ⇒ **부분 장애는 비결정적**이다. 그래서 어렵다.

## 1.1 클라우드 컴퓨팅과 슈퍼 컴퓨팅

대규모 컴퓨팅 시스템 구축 방법에 관한 몇 가지 철학이 있음.

- **고성능 컴퓨팅**: 수천 개의 CPU를 가진 슈퍼컴퓨터는 보통 일기예보 등과 같은 계산 비용이 매우 높은 과학 계산 작업에 쓰인다.
- **클라우드 컴퓨팅**: 멀티 테넌트 데이터센터, IP 네트워크로 연결된 상용 컴퓨터 등과 관련되어 있다.
- 위 두 방식은 서로 대척점에 존재하며, 전통적인 기업형 데이터센터는 이 중간 지점에 있다.

슈퍼컴퓨터에서 실행되는 작업은 보통 계산 상태를 저장소에 체크포인트로 저장한다. 그리고 노드 하나에 장애가 발생한다면, 전체 클러스터를 중단한다. 장애가 복구된 후 마지막 체크포인트부터 계산을 재시작한다.
→ 정리하면, 슈퍼컴퓨터는 분산 시스템보다는 단일 노드 컴퓨터처럼 일부 장애라도 전체가 죽게 처리한다.

분산 시스템이 동작하게 만들려면 **부분 장애 가능성을 받아들이고** 소프트웨어에 내결함성 메커니즘을 넣어야 한다.
**시스템의 어떤 부분에 결함이 생길 것이고 소프트웨어는 어떤 식으로든 그 결함을 처리**해야 한다. 
→ **결함 처리까지가 소프트웨어 설계**다. 결함을 광범위하게 고려하고 테스트 환경에서 인위적으로 이런 상황을 만들어서 어떤 일이 생기는지 보는 게 중요하다.

# 2. 신뢰성 없는 네트워크

분산 시스템은 비공유 시스템으로, 네트워크로 연결된 다수의 장비다. 

비공유 시스템으로 얻을 수 있는 이점은 아래와 같다.

- 특별한 하드웨어가 필요하지 않아서 상대적으로 저렴함
- 상품화된 클라우드 서비스를 활용할 수 있음
- 지리적으로 분산된 여러 데이터센터에 중복 배치함으로써 높은 신뢰성 확보

하지만, 네트워크 통신이다보니 언제 어디서든 장애가 발생할 수 있다. 아래처럼 여러 경우가 존재할 수 있다.

<img width="594" height="176" alt="image" src="https://github.com/user-attachments/assets/e263399a-82a6-4119-803c-163795173856" />


위와 같이 다양한 경우로 인해, 메시지가 유실될 수 있다. 공통점은 **송신 측에서 수신에 대한 응답을 못받은 것**이다.

⇒ 이 문제를 다루는 흔한 방법은 타임아웃이다.

## 2.1 결함 감지

많은 시스템은 결함 있는 노드를 자동으로 감지할 수 있어야 한다. 아래는 예시

- 로드 밸런서는 죽은 노드로 요청을 그만 보내야 한다. (즉, 죽은 노드는 순번에서 빠진 것으로 간주해야 한다.)
- 단일 리더 복제를 사용하는 분산 데이터베이스에서 리더에 장애가 나면 팔로워 중 하나가 리더로 승격돼야 한다.

네트워크에 관한 불확실성 때문에 노드가 작동 중인지 아닌지 구별하기 어렵다. 어떤 특정한 환경에서는 뭔가 동작하지 않는다고 명확하게 피드백을 받을 수도 있다.

- 수신측에 수신 대기하는 프로세스가 없다면, 수신측에서 tcp 연결을 닫거나 거부한다.
- 노드 프로세스가 죽었지만 노드의 운영체제가 실행 중이라면, 스크립트로 다른 노드에게 역할을 넘겨받을 수 있게 한다.
- 접속하려는 IP 주소에 도달할 수 없으면, 해당 라우터는 ICMP Destination Unreachable 패킷으로 응답할 수도 있다.

위와 같은 경우는 원격 노드가 다운되고 있다는 피드백으로, 매우 유용하지만 여기에 의존할 수 없다. 
→ TCP 패킷 응답을 보내더라도, 수신 애플리케이션이 그 요청을 실제로 처리하기 전에 죽을 수도 있다. 
때문에, 요청이 성공했음을 확신하고 싶다면 애플리케이션 자체로부터 긍정 응답을 받아야 한다. (HTTP - 200 ok 처럼)

## 2.2 타임아웃과 기약 없는 지연

패킷의 최대 지연 시간이 보장된 네트워크를 사용한다고 상상해보자.

모든 패킷은 특정 시간 `d` 내에 전송되거나 손실된다. 또, 장애가 나지 않는 노드는 항상 요청을 `r` 시간 내에 처리한다고 가정.
⇒ 이 경우, 성공한 요청은 모두 `2d+r` 시간 내에 응답을 받는다고 보장할 수 있다. 이 시간을 타임아웃으로 하면 간단!

현실에서 절대 불가능한 상상이다. 
비동기 네트워크는 **기약 없는 지연(unbounded delay)이 있고**, 서버 구현은 처리에 대한 최대 시간을 보장할 수 없다.
그리고, 타임아웃이 낮으면 RTT가 순간적으로 급증하기만 해도 시스템의 균형을 깨뜨린다.

## 2.3 네트워크 혼잡과 큐 대기

<img width="556" height="194" alt="image" src="https://github.com/user-attachments/assets/44790aad-5653-46fe-85bd-67dfa5237017" />


- 네트워크 스위치가 송신 큐에 패킷 넣어두고, congestion control을 수행한다. → slow start, fast recovery 등
- 또한, tcp는 flow control도 수행 → 수신버퍼 기반으로 송신량 조절한다. (receive window)

위 사진의 경우, 3번 포트에 트래픽이 몰려 스위치 큐가 가득 쌓인 상황이다. 게다가 TCP는 대기시간 안에 응답을 못받으면 패킷이 손실됐다고 간주하고 자동으로 그 패킷을 재전송한다.

이 모든 요인은 **네트워크 지연의 변동성**에 영향을 준다. 큐 대기 지연은 시스템이 최대 용량에 가까울 때 특히 광범위하게 일어난다. 예비 용량이 풍부한 시스템은 쉽게 큐를 비울 수 있지만 사용률이 높은 시스템은 긴 큐가 매우 빨리 만들어진다.

공개 클라우드나 데이터센터에서는 여러 소비자가 자원을 공유한다. 즉, 공유된 자원을 사용하기 때문에 그에 따른 네트워크 지연 변동을 예측할 수 없다. 
때문에, 이런 경우에 실험적으로 타임아웃을 선택할 수 밖에 없다. RTT 분포 측정 → **애플리케이션 특성을 고려한 장애 감지 지연과 너무 이른 타임아웃의 위험성 사이에서 적절한 트레이드오프를 결정**해야 함.

⇒ 더 좋은 방법은 시스템이 지속적으로 응답 시간과 그들의 변동성을 측정해 타임아웃을 자동으로 조절하는 것이다.

## 2.4 동기 네트워크 대 비동기 네트워크

전화 네트워크에서 통화를 할 때는 회선이 만들어진다. 통화를 하는 두 명 사이에 있는 전체 경로를 따라서 그 통화에 대해 고정되고 보장된 양의 대역폭이 할당된다. 
이 회선은 전용회선으로, 이 회선은 **통화가 끝날 때까지 유지**되며, 중간에 다른 사람이 그 경로를 사용할 수 없다. ⇒ 대역폭이 계속 보장된다.

이런 종류의 네트워크는 **동기식**이다. 데이터가 여러 라우터를 거치더라도 큐 대기 문제를 겪지 않는다. 
→ 각 네트워크 홉에 지정된 공간이 이미 할당되어 있기 때문이다. 그리고 큐 대기가 없으므로 네트워크 종단 지연 시간의 최대치가 고정돼 있다. 이를 **제한 있는 지연(bounded delay)**라고 부름

## 2.5 네트워크 지연 그냥 예측 가능하게 해줘?

패킷 기반 네트워크 계층 구조는 이게 불가능하다. TCP 연결의 본질은 네트워크 대역폭을 자유자재로 사용한다는 점이다.

인터넷은 기본적으로 패킷 교환 방식을 사용한다. 이는 **순간적으로 몰리는 트래픽에 최적화됐기 때문**이다.
트래픽 성격을 분석해보면, 구분이 쉬움.

**회선교환(통화):** 지연이 예측 가능하고 당연히 고품질임. 근데, 유연하지 못한다. 리소스 낭비하는 셈이니..

**패킷교환(인터넷):** 자원을 공유하니 확장성에 용이하다. 하지만, 패킷마다 경로가 다를 수 있음(⇒ 순서 불일치, 지연, 손실 등… 문제 겁나 많음)

# 3. 신뢰성 없는 시계

애플리케이션은 아래와 같은 질문을 답하기 위해 다양한 방식으로 시계에 의존한다.

1. 이 요청이 타임아웃 됐나?
2. 이 서비스의 99분위 응답 시간은 어떻게 되나?
3. 이 서비스는 지난 5분 동안 평균 초당 몇 개의 질의를 처리했나?
4. 사용자가 우리 사이트에서 시간을 얼마나 보냈나?
5. 이 기사가 언제 게시됐나?
6. 며칠 몇 시에 미리 알림 이메일을 보내야 하나?
7. 이 캐시 항목은 언제 만료되나?
8. 로그 파일에 남은 이 오류 메시지의 타임스탬프는 무엇인가?

1~4는 **지속 시간**을 측정하는 반면, 5~8은 **시점**을 기술한다.

분산 시스템에서 메시지를 받은 시간은 항상 보낸 시간보다 나중이지만 네트워크의 지연 변동성 때문에 얼마나 나중일지는 알 수 없다. 게다가 네트워크에 속해있는 개별 장비는 자신의 시계를 갖고 있다. 이때 각 장비마다 시간을 동기화할 필요가 있다.

⇒ 이때 가장 널리 쓰이는 메커니즘이 NTP(Network Time Protocol)로, 네트워크 상의 장치들 간 정확한 시간 동기화를 진행한다.

## 3.1 단조 시계 vs 일 기준 시계

현대 컴퓨터는 최소 두 가지 종류의 시계를 가진다. 

- 일 기준 시계(time-of-day clock)
- 단조 시계(monotonic clock)

### 3.1.1 일 기준 시계

**사람이 보는 실제 날짜/시간(년, 월, 일, 시, 분, 초)**을 표현한다. 시간 기준은 **UTC 기반**, Epoch 기준 (1970-01-01 00:00:00 UTC부터 흐른 시간)이다.

일 기준 시계는 보통 NTP로 동기화한다. 한 장비의 타임스탬프는 다른 장비의 타임스탬프와 동일한 의미를 지닌다는 뜻이다. 특히, 로컬 시계가 NTP 서버보다 너무 앞서면 강제로 리셋되어 과거 시점으로 거꾸로 뛰는 것처럼 보일 수 있다. → 이에 따라 시간 간격을 측정하는 데는 부적합

### 3.1.2 단조 시계

이는 타임아웃이나 서비스 응답 시간 같은 **지속 시간**을 재는 데 적합하다. 시작 이후 **항상 증가만** 하는 시간값을 제공한다.

시간 기준은 **어느 시점**이다. 즉, 네트워크 시간 동기화와 무관하다.

## 3.2 시계 동기화와 정확도

단조 시계와 달리 일 기준 시계는 NTP 서버나 다른 외부 시간 출처에 맞춰 설정돼야 유용하다. 시계가 정확한 시간을 알려주게 하는 방법은 기대만큼 신뢰성이 있지 않다.

- 컴퓨터 시계가 NTP 서버와 너무 많은 차이가 나면 **동기화가 거부**되거나 로컬 시계가 강제로 리셋될 수도 있다. 리셋 전후에 시간을 관찰한 애플리케이션은 시간이 거꾸로 흐르거나 갑자기 앞으로 뛰는 것을 볼지도 모른다.
- 뜻하지 않게 노드와 NTP 서버 사이가 방화벽으로 막히면 잘못된 설정이 얼마동안 알려지지 않을 수도 있다.
- NTP 동기화는 잘해봐야 네트워크 지연만큼만 좋을 수 있다. 따라서 패킷 지연의 변화가 큰 네트워크 환경에서는 정확도가 떨어질 수 있다. 설정에 따라 네트워크 지연이 크면 NTP 클라이언트가 완전히 포기할 수도 있다.
- 윤초 발생하면 엉망이 된다. NTP 서버에서 윤초를 고려해서 설계되어야 한다.

## 3.3 동기화된 시계에 의존하기

시계는 대부분 시간에 아주 잘 동작하지만 견고한 소프트웨어는 잘못된 시계에 대비할 필요가 있다. 

서버의 시계에 결함이 있거나 NTP 클라이언트가 잘못 설정됐다면 시계는 드리프트가 생긴다. 즉, 점점 실제 시간으로부터 멀어져간다.. → 근데 이게 실제로 파악하기는 어렵다. ⇒ 그 결과 조용하고 미묘한 데이터 손실이 발생하게 된다.

⇒ 따라서, 모니터링을 통해 다른 노드와 너무 차이가 나는 노드는 죽은 것으로 선언되고 클러스터에서 제거돼야 한다.

## 3.4 이벤트 순서화용 타임스탬프

<img width="594" height="260" alt="image" src="https://github.com/user-attachments/assets/b789e9b1-988b-4541-8c35-8d8285b72a4e" />


위 그림은 다중 리더 복제를 쓰는 데이터베이스에서 일 기준 시간을 위험하게 사용하는 예다. 

- 클라이언트 A가 노드 1에 `x = 1`을 씀.
- 그 쓰기는 노드 3 → 노드 2 순서로 복제되고 있다.

실제 물리적인 x 값 업데이트는 노드 3에서 `x = 2` 를 한게 가장 최근이다. 

근데, 노드 2 관점에서 살펴보면 아래와 같음

1. 42.003초 타임스탬프 값과 함께 x는 `2`로 업데이트 받음.
2. 뒤늦게 42.004초 타임스탬프 값과 함께 x는 `1`로 업데이트 받음.

이 결과 노드 2는 `1`이란 값이 더 최근 값으로 결정하게 되어, 클라이언트 B의 증가 연산이 손실된다.

이 충돌 해소 전략은 **최종 쓰기 승리(last write wins, LWW)**라고 불리며 다중 리더 복제와, 리더 없는 데이터베이스에서 널리 사용된다.

LWW은 결국 가장 나중에 기록된 값을 최종값으로 선택하는 전략이다. ⇒ 논리적 오류(인과성 위반) 발생 가능.

시간 기반으로 처리할 때에 “최근”의 정의를 시계로 처리하면 논리적 오류가 발생할 가능성이 있다는 것을 항상 인지해야 함.
⇒ 예를 들어, 엄격하게 NTP로 동기화된 시계를 쓰더라도 (송신 측에서) 타임스탬프 100밀리초에 패킷을 보내고, (수신 측에서) 타임스탬프 99밀리초에 패킷을 받을 수 있다. (→ 논리적으로 패킷을 보내기도 전에 도착했다는 말인데, 불가능..)

NTP 동기화는 결국 네트워크를 타기 때문에 “완벽”이 불가능하다.
⇒ 올바른 순서화를 위해서는 **시계 출처가 측정하려고 하는 대상(즉 네트워크 지연)보다 훨씬 더 정확해야 한다.**

**결국 이벤트 순서화는 이러한 물리적 시계보다는 이벤트의 상대적인 순서를 기반으로 보장해야 한다. → 뭐 이걸 논리적 시계(logical clock)라고 표현한다고 함.**

## 3.5 시계 읽기는 신뢰 구간이 있다

로컬 네트워크에 있는 NTP 서버와 매분 동기화하더라도 부정확한 수정 시계에서 발생하는 드리프트는 쉽게 몇 밀리초가 될 수 있다. 따라서 시계 읽기를 어떤 시점으로 생각하는 것은 타당하지 않다. 어떤 신뢰 구간에 속하는 시간의 범위로 읽는게 나을 것이다.
⇒ 시간이 +/- 100밀리초 범위 내에 있는 것을 알면 타임스탬프에 있는 마이크로초 단위 숫자는 그다지 의미가 없다.

시계의 신뢰도는 어디에서 시간을 가져왔느냐에 따라 달라진다.

예를 들어,

- GPS 수신기 같은 건 **위성에서 받은 매우 정밀한 원자시계 신호**를 쓰기 때문에 오차 범위가 작다.
- 반면, **NTP 서버**를 통해 인터넷으로 시간을 받으면 네트워크 지연 때문에 수 밀리초~수백 밀리초까지 오차가 생긴다.

즉, 서버가 어떤 방식으로 시간을 얻었느냐에 따라 신뢰 구간(불확실성의 범위)이 달라진다.
하지만, 대부분의 시스템은 이 오차범위를 노출하지 않음.

예를 들어, 

리눅스에서 `clock_gettime()` 을 호출하면 “현재 시간이 10.3초 라는 것만 알려주지, “±5밀리초 오차일 수도 있다”는 정보는 안준다.

흥미로운 예외는 구글 트루타임 API다. 구글의 Spanner 시스템이 유일하게 이 불확실성을 명시적으로 다루는 예시다.

**Google TrueTime API**는 시간을 단일 값이 아니라 “구간(range)”으로 반환.

- earliest = 10.300100초
- latest   = 10.300200초

이게 바로 신뢰 구간이다. 실제 현재 시간이 요 범위 안의 어딘가에 있다는 것을 의미한다.

## 3.6 전역 스냅샷용 동기화된 시계

스냅샷 격리는 잠금을 쓰지 않고 다른 트랜잭션을 방해하지 않으면서 읽기 전용 트랜잭션이 특정 시점의 일관된 상태의 데이터베이스를 볼 수 있게 한다.

가장 흔한 구현 방식은 단조 증가하는 트랜잭션 ID를 기반으로 선후관계를 정의하는 것이다.
그러나, 데이터베이스가 여러 장비로 분산돼 있을 때는 코디네이션이 필요하다. → **모든 파티션에 걸쳐서 전역 단조 증가 ID를 만들어야 하므로.**
⇒ 트랜잭션 ID는 인과성을 반영해야 한다. 그렇지 않으면 스냅샷이 일관성을 지니지 못한다. 또한, 이 트랜잭션ID 생성은 방어할 수 없는 병목이 된다.

⇒ **일 기준 시계의 타임스탬프를 트랜잭션 ID를 쓸 수 있지 않을까 싶을 수 있다. 동기화가 잘되면 베스트다.** 
**하지만, 문제는 시계 정확도의 불확실성이다.**

**구글 스패너의 스냅샷 구현은 트루타입 API가 보고한 시계 신뢰 구간을 기반으로 되어있다.**

TrueTime은 “정확한 시각” 대신 **시간의 범위(earliest~latest)**를 알려준다. ⇒ 즉, “현재 시각은 이 사이 어딘가”라는 식

그래서 두 트랜잭션의 시간 범위가 **겹치지 않으면** 순서를 확실히 알 수 있습니다. ⇒ 하지만 **겹치면** 누가 먼저인지 확실하지 않음.

그래서 Spanner는 트랜잭션을 커밋할 때 **신뢰 구간이 겹치지 않을 만큼 잠시 기다린다. ⇒** 이렇게 하면 “이 트랜잭션은 이전 것보다 확실히 나중에 실행됐다”는 걸 보장할 수 있기 때문.

## 3.7 프로세스 중단

리더-팔로워 구조를 예시로, 시계가 틀리거나 스레드가 잠깐 멈추면 분산 시스템이 리더를 잘못 판단해 큰 문제가 생길 수 있다.

```java
while (true) {
    request = getIncomingRequest();

    // 만료까지 10초 이하로 남으면 갱신
    if (lease.expiryTimeMillis - System.currentTimeMillis() < 10000) {
        lease = lease.renew();
    }

    // 유효하면 요청 처리
    if (lease.isValid()) {
        process(request);
    }
}
```

- 분산 데이터베이스에서 **한 시점에 하나의 리더만 쓰기 요청을 처리**해야 한다.
- 이를 위해 각 노드는 **리더 임차권(lease)**을 얻음.
    
    → 예: “나는 앞으로 30초 동안 리더 역할을 맡겠다.”
    
- 리더는 임차권이 만료되기 전에 **주기적으로 갱신**해야 하고, 장애가 생겨 갱신하지 못하면 다른 노드가 새 리더가 된다.

### 문제 1. 시계에 의존하면 위험함

- 이 코드는 로컬 시스템 시계(clock)를 기준으로 lease 만료를 계산한다.
- 그런데 시스템 시계는 NTP 동기화나 설정 오류로 **앞뒤로 움직일 수 있음**.
    
    → 시계가 갑자기 “앞으로 뛸 경우” lease가 조기 만료된 것처럼 보임
    
    → 반대로 “뒤로 가면” 이미 만료된 lease를 계속 유효하다고 착각할 수도 있음
    

**결과:** 잘못된 리더가 계속 쓰기를 처리하는 “리더 중복” 발생 가능 (데이터 무결성 깨짐)

### 문제 2. 프로세스(스레드)가 멈출 수 있는 경우

코드가 정상적으로 주기 실행된다고 가정하지만, 실제 시스템에서는 스레드가 예기치 않게 **잠시 멈출 수 있음.**
(stop-the-world 현상 등)

`lease.isValid()` 호출 전에 프로세스가 멈췄다고 가정해보자. **이 경우 처리하는 시점에 만료돼서 다른 노드가 이미 리더 역할을 넘겨받았을 가능성이 높다.**

예시)

- **GC(Garbage Collector)**: JVM이 stop-the-world를 일으켜 수 초 동안 멈출 수 있음
- **가상화/클라우드 suspend**: VM이 잠깐 멈추고 나중에 resume될 수 있음
- **디스크 I/O 지연, 스왑, 페이징**: 커널이 스레드를 일시 정지시킬 수 있음
- **운영체제 스케줄링**: CPU를 다른 프로세스가 가져가면서 코드가 중단됨
- **유닉스 SIGSTOP 신호**: 프로세스가 수동으로 일시정지될 수도 있음

결론적으로, 단일 장비에서 스레드 안전(thread-safe)을 지키는 건 **락(lock)** 등으로 가능하지만, 분산 시스템에서는 **공유 메모리가 없고, 오직 메시지로만 통신**하므로 이런 “시간 기반 판단 오류”를 막기가 훨씬 어렵다.

## 3.8 응답 시간 보장

**특정 시간(deadline)** 안에 반드시 응답해야 하는 실시간 시스템에서는 위와 같은 중단을 없애야 한다.

실시간을 위한 필요한 기술 요소는 아래와 같다.

- **RTOS (Real-Time Operating System)**:
    
    프로세스에게 **정확한 CPU 시간 보장**을 해주는 운영체제.
    
    → 일반 OS와 달리, “언제 실행될지”가 아니라 “언제까지 반드시 끝나야 한다”를 중심으로 스케줄링함.
    
- **하드웨어 제약 강화:**
    
    메모리 동적 할당 제한, 가비지 컬렉션 금지 또는 예측 가능한 GC만 허용.
    
- **라이브러리/함수의 Worst Case 실행 시간(WCET)** 측정.
    
    → “이 함수는 최대 2ms 걸린다”처럼 예측 가능하게 해야 함.
    
- **테스트/측정 중심 개발:**
    
    모든 연산의 시간적 상한선을 명확히 알고 있어야 함.
    

하지만, 이런 **실시간 보장**을 달성하려면,
→ 비용, 개발 노력, 시스템 복잡도가 매우 커진다.
→ 그래서 현실적으로 **항공·로봇·임베디드 분야** 같은 특수 영역에서만 사용한다.

**서버·웹 시스템은 경제성, 효율성, 유연성**이 중요하기 때문에,
→ 완전한 실시간 보장은 거의 불가능하고,
→ 대신 “대체로 빠르고 안정적인 응답”을 목표로 함 (**soft real-time** 수준).

정리) **하드 실시간 시스템은 정해진 시간 안에 반드시 응답해야 하는 시스템이고, 일반 서버 시스템은 그런 보장을 포기하고 유연성과 효율성을 택한다.**

# 4. 지식, 진실, 그리고 거짓말

분산 시스템에서 우리는 동작(시스템 모델)에 관해 정한 가정을 명시하고, 이런 가정을 만족시키는 방식으로 실제 시스템을 설계할 수 있다. 그러나, **신뢰성 없는 시스템 모델에서 잘 동작하는 소프트웨어를 만드는 게 가능할지라도 그것이 간단하지는 않다.**

## 4.1 진실은 다수결로 결정된다

분산 시스템에서는 한 노드에만 의존할 수 없다. 즉, 노드가 상황에 대한 자신의 판단을 반드시 믿으면 안된다. (나도 믿으면 안됨)
⇒ 노드에 언제든 장애가 나서 잠재적으로 시스템이 멈추고 복구할 수 없게 될 수도 있기 때문이다.

때문에, 여러 분산 알고리즘은 정족수를 정해두고 투표를 해서 판단한다. 예를 들어, 노드가 죽었다고 선언하는 것에 관해 정족수를 이룬 결정을 기반으로 죽여야 한다.

## 4.2 리더와 잠금

시스템이 오직 하나의 뭔가가 필요할 때가 있다.

예)

- 스플릿 브레인을 피하기 위해 오직 하나의 노드만 데이터베이스 파티션의 리더가 될 수 있다.
- 특정한 자원에 접근하는데 동시성을 제어하기 위해, 오직 하나의 트랜잭션이나 클라이언트만 잠금을 획득할 수 있다.

**분산 시스템에서 이를 구현하려면 주의**해야 한다.

어떤 노드가 스스로 위 예시 상황에서 그 리더라고 믿을지라도 정족수도 동의하는게 아니다. 리더가 죽고나서 다른 리더가 선출된 와중에, 죽은 리더가 다시 살아나면? ⇒ 스플릿 브레인

아래 예시는 저장 서비스에 있는 어떤 파일을 한 번에 한 클라이언트만 접근하도록 보장하고 싶은데, 잘못된 예시다.

<img width="592" height="223" alt="image" src="https://github.com/user-attachments/assets/14984c0c-7129-4666-9c07-f4817d4dc54f" />


클라이언트 1이 락을 획득한 와중에 프로세스가 중단되었고, 락 타임아웃에 의해 클라이언트 2에게 락이 넘어간 상황이다.

⇒ 클라이언트 1은 아직도 락을 획득한 상태라고 인지하고 데이터를 기록한다. 클라 2의 기록이 오염됨.

## 4.3 펜싱 토큰

공유 자원에 대한 접근을 보호하기 위해 잠금을 사용할 때, 자신이 “선택된 자”라고 잘못 믿는 현상을 방지하기 위해 **펜싱(fencing)**이란 단순한 기법을 사용할 수 있다.

<img width="615" height="221" alt="image" src="https://github.com/user-attachments/assets/f49640ca-7d87-4050-b996-05e44d47fa62" />


잠금 서비스가 잠금을 승인할 때마다 **펜싱 토큰(fencing token)**을 반환한다고 가정한다. 펜싱 토큰은 잠금이 승인될 때마다 증가하는 숫자다. 이는 잠금이 승인될 때마다 증가하는 숫자다. 
⇒ 클라이언트가 쓰기 요청을 저장소로 보낼 때마다 자신의 현재 토큰을 포함하도록 요구할 수 있다.
⇒ 저장소는 쓰기가 반영된 최근 토큰 번호를 기억하고, 이전 토큰은 처리를 거부한다.

*잠금 서비스로 주키퍼를 사용하면 트랜잭션 ID를 zxid나 노드 버전 cversion을 펜싱 토큰으로 사용할 수 있다고 함. 
⇒ 단조 증가가 보장되므로.*

## 4.4 비잔틴 결함

펜싱 토큰은 (자신의 락이 만료되었다는 것을 아직 알아채지 못하는 등의) **부주의에 의한 오류**에 빠진 노드를 감지하고 차단할 수 있다.

이런 식으로 분산 시스템에서는 노드가 ‘거짓말’을 할지도 모른다는 위험에 노출되어 있다. 예를 들어, 어떤 노드가 실제로 받지 않은 특정 메시지를 받았다고 주장할 수도 있는 것이다. 

⇒ **이런 동작을 비잔틴 결함(Byzantine fault)이라고 하며, 이런 신뢰가 없는 환경에서 합의에 도달하는 문제를 비잔틴 장군 문제라고 한다.**

*이 책에서는 노드가 고의로 시스템의 보장을 무너뜨리기 위해 펜싱 토큰을 조작하면 막을 수 없다. 
⇒ 노드들이 정직하다는 가정 하에, 응답만 한다면 그 노드는 ‘진실’을 말한다고 가정.*

이러한 비탄진 결함이 있더라도 시스템이 계속 올바르게 동작한다면, 이 시스템은 **비잔틴 내결함성을 지닌다.**

일반적인 서버 시스템에서는 비잔틴 내결함성을 지닐 필요성이 좀 떨어진다. 대부분의 데이터센터에서는 **모든 노드가 신뢰 가능한 한 조직의 통제 하에 있기** 때문이다. 즉, 단순한 장애(노드 다운, 네트워크 끊김)만 처리하면 충분하다.

비잔틴 내결함성을 구현하려면 프로토콜이 매우 복잡하고, 성능이 떨어지며 비용이 크다.
⇒ 항공, 군사 같은 극히 일부 시스템만 사용한다.

## 4.5 시스템 모델과 현실

분산 시스템 문제를 해결하기 위해 많은 알고리즘이 설계되고 있다. 예를 들어, 합의 알고리즘.
⇒ 이 알고리즘은 분산 시스템의 다양한 결함을 견딜 수 있어야 하며, **하드웨어와 소프트웨어 설정에 너무 심하게 의존되지 않아야 한다.**

즉, 시스템에서 발생한 것으로 예상되는 결함의 종류를 **(시스템 모델을 정의해서)어떻게든 정형화**해야 한다. 

타이밍 이슈 관점에서 보면, 아래 세 가지로 모델을 분류할 수 있다.

- **동기식 모델**
    
    네트워크 지연, 프로세스 중단, 시계 오차에 모두 제한이 있다고 가정한다. 아예 없다는 것은 아니고, 어떤 고정된 상한치가 존재한다는 가정! 
    
    → 동기식 모델은 기약 없는 지연과 중단이 발생하기 때문에, 현실적인 모델은 아니다.
    
- **부분 동기식 모델**
    
    부분 동기는 시스템이 대부분의 시간에는 동기식 시스템처럼 동작하지만 때때로 네트워크 지연, 프로세스 중단, 시계 드리프트의 한계치를 초과하는 의미다.
    
- **비동기식 모델**
    
    타이밍에 대한 어떤 가정도 할 수 없다. 
    

노드 관점에서 보면, 아래와 같이 시스템 모델을 분류할 수 있다.

- **죽으면 중단하는 모델(crash-stop)**
    
    **노드에 장애가 나는 것은 죽는 것 하나 라고 가정**할 수 있다. 노드가 어느 순간부터 응답을 멈추면 영원히 사용할 수 없다는 뜻이다.
    
- **죽으면 복구하는 모델(crash-recovery)**
    
    노드가 어느 순간에 죽을 수는 있지만 시간이 흐른 후에 다시 응답하기 시작할 것이라고 가정한다. 죽다 살아나면 메모리는 날라가더라도 저장소에 데이터가 남아있다고 가정한다.
    
- **비잔틴 결함**
    
    다른 노드를 속이는 것을 포함해 전적으로 무슨 일이든 할 수 있다.
    

현실 시스템에서 가장 일반적이고 유용한 모델은 “죽으면 복구하는 결함을 지닌 부분 동기식”모델 이다.
⇒ 분산 알고리즘이 이 모델에 어떻게 대응??

## 4.6 알고리즘의 정확성

알고리즘이 정확하다는게 어떤 의미인지 정의하기 위해 속성을 나열할 수 있다.

예를 들어, **잠금에 사용할 펜싱 토큰을 생성하는 알고리즘의 속성**은 아래와 같다.

- **유일성**
    
    펜싱 토큰 요청이 같은 값을 반환하지 않는다.
    
- **단조 일련번호**
    
    요청 y가 시작하기 전에 요청 x가 완료되었다면, y의 토큰 번호가 더 커야 한다.
    
- **가용성**
    
    펜싱 토큰을 요청하고 죽지 않은 노드는 결국에는 응답을 받는다.
    

근데, 모든 노드가 죽거나 모든 네트워크가 무한히 지연되면 어떤 알고리즘이라도 아무것도 못한다.
